{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CounterGAN Tabular Fine-Tune (German Credit)\n",
        "\n",
        "This notebook implements:\n",
        "1. Clustering of users into profiles.\n",
        "2. Loading of cluster-specific immutable features & cost weights.\n",
        "3. Personalized CounterGAN training per cluster.\n",
        "4. Generation and evaluation of counterfactuals (baseline vs personalized).\n",
        "5. Hyperparameter tuning (randomized grid search).\n",
        "6. Saving the final recommended profile configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 10:55:05.362571: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-07-10 10:55:05.407449: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-07-10 10:55:05.407482: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-07-10 10:55:05.407518: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-10 10:55:05.417068: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-07-10 10:55:05.417749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-07-10 10:55:06.354242: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import itertools\n",
        "import jsonpickle\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()   # TF1-style eager mode\n",
        "\n",
        "from datetime import datetime\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "from tensorflow.keras import Model, optimizers\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Add, Input, ActivityRegularization, Dropout\n",
        "# Alibi logger\n",
        "import logging\n",
        "alibi_logger = logging.getLogger('alibi')\n",
        "alibi_logger.setLevel('INFO')\n",
        "\n",
        "BASE_PATH = \"./counterfactuals\"\n",
        "\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "\n",
        "\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_{date}\"\n",
        "MODELS_EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_2020-09-09\"\n",
        "if not os.path.exists(EXPERIMENT_PATH):\n",
        "    os.makedirs(EXPERIMENT_PATH)\n",
        "    \n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status               →  1 cols @ indices [0]\n",
            "Month                →  1 cols @ indices [1]\n",
            "Credit_History       →  1 cols @ indices [2]\n",
            "Purpose              →  9 cols @ indices [11, 12, 13, 14, 15]…\n",
            "Credit_Amount        →  1 cols @ indices [3]\n",
            "Savings              →  1 cols @ indices [4]\n",
            "Employment           →  1 cols @ indices [5]\n",
            "Installment_Rate     →  1 cols @ indices [6]\n",
            "Personal_Status      →  3 cols @ indices [20, 21, 22]\n",
            "Other_Debtors        →  2 cols @ indices [23, 24]\n",
            "Residence_Duration   →  1 cols @ indices [7]\n",
            "Property             →  3 cols @ indices [25, 26, 27]\n",
            "Age                  →  1 cols @ indices [8]\n",
            "Other_Installment_Plans →  2 cols @ indices [28, 29]\n",
            "Housing              →  2 cols @ indices [30, 31]\n",
            "Existing_Credits     →  1 cols @ indices [9]\n",
            "Job                  →  3 cols @ indices [32, 33, 34]\n",
            "Num_Liable_People    →  1 cols @ indices [10]\n",
            "Telephone            →  1 cols @ indices [35]\n",
            "Foreign_Worker       →  1 cols @ indices [36]\n"
          ]
        }
      ],
      "source": [
        "INITIAL_CLASS = 0\n",
        "DESIRED_CLASS = 1\n",
        "N_CLASSES = 2\n",
        "# n_training_iterations = 10\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.set_printoptions(precision=2)\n",
        "random.seed(2020)\n",
        "np.random.seed(2020)\n",
        "tf.random.set_seed(2020)\n",
        "\n",
        "# German Credit dataset\n",
        "\n",
        "def preprocess_data_german(df, target_column=\"Outcome\"):\n",
        "    \"\"\"\n",
        "    Preprocess the German Credit dataset by encoding categorical variables and splitting the data into \n",
        "    train, test, and user simulation sets.\n",
        "    \n",
        "    Returns a dictionary with processed train, test, and user datasets.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Assign meaningful column names\n",
        "    df.columns = [\n",
        "        'Status', 'Month', 'Credit_History', 'Purpose', 'Credit_Amount',\n",
        "        'Savings', 'Employment', 'Installment_Rate', 'Personal_Status', 'Other_Debtors',\n",
        "        'Residence_Duration', 'Property', 'Age', 'Other_Installment_Plans', 'Housing',\n",
        "        'Existing_Credits', 'Job', 'Num_Liable_People', 'Telephone', 'Foreign_Worker',\n",
        "        'Outcome'\n",
        "    ]\n",
        "    \n",
        "    # Mapping categorical features to more meaningful values\n",
        "    status_mapping = { 'A11': '< 0 DM', 'A12': '0 <= ... < 200 DM', 'A13': '>= 200 DM / salary assignments for at least 1 year', 'A14': 'no checking account' }\n",
        "    credit_history_mapping = { 'A30': 'no credits taken/ all credits paid back duly', 'A31': 'all credits at this bank paid back duly', 'A32': 'existing credits paid back duly till now', 'A33': 'delay in paying off in the past', 'A34': 'critical account/other credits existing' }\n",
        "    savings_mapping = { 'A61': '< 100 DM', 'A62': '100 <= ... < 500 DM', 'A63': '500 <= ... < 1000 DM', 'A64': '>= 1000 DM', 'A65': 'unknown/no savings account' }\n",
        "    employment_mapping = { 'A71': 'unemployed', 'A72': '< 1 year', 'A73': '1 <= ... < 4 years', 'A74': '4 <= ... < 7 years', 'A75': '>= 7 years' }\n",
        "    personal_status_mapping = { 'A91': 'male: divorced/separated', 'A92': 'female: divorced/separated/married', 'A93': 'male: single', 'A94': 'male: married/widowed', 'A95': 'female: single' }\n",
        "    other_debtors_mapping = { 'A101': 'none', 'A102': 'co-applicant', 'A103': 'guarantor' }\n",
        "    property_mapping = { 'A121': 'real estate', 'A122': 'building society savings agreement/life insurance', 'A123': 'car or other, not in attribute 6', 'A124': 'unknown/no property' }\n",
        "    other_installment_plans_mapping = { 'A141': 'bank', 'A142': 'stores', 'A143': 'none' }\n",
        "    housing_mapping = { 'A151': 'rent', 'A152': 'own', 'A153': 'for free' }\n",
        "    telephone_mapping = { 'A191': 'none', 'A192': 'yes, registered under the customer\\'s name' }\n",
        "    foreign_worker_mapping = { 'A201': 'yes', 'A202': 'no' }\n",
        "\n",
        "    # Apply mappings\n",
        "    df['Status'] = df['Status'].map(status_mapping)\n",
        "    df['Credit_History'] = df['Credit_History'].map(credit_history_mapping)\n",
        "    df['Savings'] = df['Savings'].map(savings_mapping)\n",
        "    df['Employment'] = df['Employment'].map(employment_mapping)\n",
        "    df['Personal_Status'] = df['Personal_Status'].map(personal_status_mapping)\n",
        "    df['Other_Debtors'] = df['Other_Debtors'].map(other_debtors_mapping)\n",
        "    df['Property'] = df['Property'].map(property_mapping)\n",
        "    df['Other_Installment_Plans'] = df['Other_Installment_Plans'].map(other_installment_plans_mapping)\n",
        "    df['Housing'] = df['Housing'].map(housing_mapping)\n",
        "    df['Telephone'] = df['Telephone'].map(telephone_mapping)\n",
        "    df['Foreign_Worker'] = df['Foreign_Worker'].map(foreign_worker_mapping)\n",
        "    df_pre_encoding = df.copy()\n",
        "\n",
        "    # Encode ordinal columns\n",
        "    ordinal_cols = ['Status', 'Credit_History', 'Savings', 'Employment']\n",
        "    le = LabelEncoder()\n",
        "    for col in ordinal_cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # One-hot encode nominal columns\n",
        "    nominal_columns = ['Purpose', 'Personal_Status', 'Other_Debtors', 'Property', \n",
        "                       'Other_Installment_Plans', 'Housing', 'Job', 'Telephone', 'Foreign_Worker']\n",
        "    df = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
        "\n",
        "    # Process target variable\n",
        "    Y = df[target_column].replace(1, 0).replace(2, 1)\n",
        "    X = df.drop(columns=[target_column])\n",
        "\n",
        "    # Get final feature set\n",
        "    # list all features\n",
        "    # immutable_features = set(X.columns) - set(['Status', 'Credit_History'])\n",
        "    \n",
        "\n",
        "    # mutable_features = set(X.columns) - set(immutable_features)\n",
        "    # mutable_features = list(mutable_features)\n",
        "\n",
        "    features = list(set(X.columns))\n",
        "\n",
        "    return  X, Y, features, df_pre_encoding\n",
        "    \n",
        "# =========================================================\n",
        "\n",
        "\n",
        "# Make sure 'german.csv' is in your project directory\n",
        "df = pd.read_csv('statlog_german_credit_data/german.data', sep=' ', skiprows=1, header=None)\n",
        "# 1) Run your existing preprocessing\n",
        "X, Y, features, df_pre_encoding = preprocess_data_german(df)\n",
        "\n",
        "# 2) Grab the list of encoded column names (in order) from X\n",
        "encoded_cols = list(X.columns)\n",
        "\n",
        "# 3) Get your “raw” feature names, i.e. the columns in df_pre_encoding before one-hot\n",
        "raw_cols = [c for c in df_pre_encoding.columns if c != \"Outcome\"]\n",
        "\n",
        "# 4) Build the mapping raw_feature → list of encoded indices\n",
        "raw_to_encoded = {}\n",
        "for raw in raw_cols:\n",
        "    # any encoded column exactly equal (for your ordinal cols)\n",
        "    # or starting with “raw_” (for one-hot dummies)\n",
        "    idxs = [\n",
        "        i for i, c in enumerate(encoded_cols)\n",
        "        if c == raw or c.startswith(raw + \"_\")\n",
        "    ]\n",
        "    raw_to_encoded[raw] = idxs\n",
        "\n",
        "# 5) (Optional) Inspect\n",
        "for raw, idxs in raw_to_encoded.items():\n",
        "    print(f\"{raw:20s} → {len(idxs):2d} cols @ indices {idxs[:5]}{'…' if len(idxs)>5 else ''}\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=2020)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "X_train = standard_scaler.fit_transform(X_train)\n",
        "X_test = standard_scaler.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c3506c03",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status               →  1 encoded columns:\n",
            "    [  0] Status\n",
            "\n",
            "Month                →  1 encoded columns:\n",
            "    [  1] Month\n",
            "\n",
            "Credit_History       →  1 encoded columns:\n",
            "    [  2] Credit_History\n",
            "\n",
            "Purpose              →  9 encoded columns:\n",
            "    [ 11] Purpose_A41\n",
            "    [ 12] Purpose_A410\n",
            "    [ 13] Purpose_A42\n",
            "    [ 14] Purpose_A43\n",
            "    [ 15] Purpose_A44\n",
            "    [ 16] Purpose_A45\n",
            "    [ 17] Purpose_A46\n",
            "    [ 18] Purpose_A48\n",
            "    [ 19] Purpose_A49\n",
            "\n",
            "Credit_Amount        →  1 encoded columns:\n",
            "    [  3] Credit_Amount\n",
            "\n",
            "Savings              →  1 encoded columns:\n",
            "    [  4] Savings\n",
            "\n",
            "Employment           →  1 encoded columns:\n",
            "    [  5] Employment\n",
            "\n",
            "Installment_Rate     →  1 encoded columns:\n",
            "    [  6] Installment_Rate\n",
            "\n",
            "Personal_Status      →  3 encoded columns:\n",
            "    [ 20] Personal_Status_male: divorced/separated\n",
            "    [ 21] Personal_Status_male: married/widowed\n",
            "    [ 22] Personal_Status_male: single\n",
            "\n",
            "Other_Debtors        →  2 encoded columns:\n",
            "    [ 23] Other_Debtors_guarantor\n",
            "    [ 24] Other_Debtors_none\n",
            "\n",
            "Residence_Duration   →  1 encoded columns:\n",
            "    [  7] Residence_Duration\n",
            "\n",
            "Property             →  3 encoded columns:\n",
            "    [ 25] Property_car or other, not in attribute 6\n",
            "    [ 26] Property_real estate\n",
            "    [ 27] Property_unknown/no property\n",
            "\n",
            "Age                  →  1 encoded columns:\n",
            "    [  8] Age\n",
            "\n",
            "Other_Installment_Plans →  2 encoded columns:\n",
            "    [ 28] Other_Installment_Plans_none\n",
            "    [ 29] Other_Installment_Plans_stores\n",
            "\n",
            "Housing              →  2 encoded columns:\n",
            "    [ 30] Housing_own\n",
            "    [ 31] Housing_rent\n",
            "\n",
            "Existing_Credits     →  1 encoded columns:\n",
            "    [  9] Existing_Credits\n",
            "\n",
            "Job                  →  3 encoded columns:\n",
            "    [ 32] Job_A172\n",
            "    [ 33] Job_A173\n",
            "    [ 34] Job_A174\n",
            "\n",
            "Num_Liable_People    →  1 encoded columns:\n",
            "    [ 10] Num_Liable_People\n",
            "\n",
            "Telephone            →  1 encoded columns:\n",
            "    [ 35] Telephone_yes, registered under the customer's name\n",
            "\n",
            "Foreign_Worker       →  1 encoded columns:\n",
            "    [ 36] Foreign_Worker_yes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# assume encoded_cols and raw_to_encoded are already defined from the previous step\n",
        "\n",
        "for raw, idxs in raw_to_encoded.items():\n",
        "    print(f\"{raw:20s} → {len(idxs):2d} encoded columns:\")\n",
        "    for i in idxs:\n",
        "        print(f\"    [{i:3d}] {encoded_cols[i]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bc17a3cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier loaded from ./counterfactuals/diabetes_2025-07-10/classifier.keras\n",
            "Autoencoder loaded from ./counterfactuals/diabetes_2025-07-10/autoencoder.keras\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the classifier model\n",
        "filename = f\"{EXPERIMENT_PATH}/classifier.keras\"\n",
        "classifier = load_model(filename)\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "classifier.trainable = False\n",
        "print(f\"Classifier loaded from {filename}\") \n",
        "\n",
        "# Load the autoencoder model\n",
        "filename = f\"{EXPERIMENT_PATH}/autoencoder.keras\" \n",
        "autoencoder = load_model(filename)\n",
        "# Ensure the autoencoder is compiled with the same optimizer and loss function  \n",
        "autoencoder.compile(optimizer='nadam', loss='mse')\n",
        "\n",
        "print(f\"Autoencoder loaded from {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_reconstruction_error(x, autoencoder):\n",
        "    \"\"\"Compute ∞-norm reconstruction error via autoencoder.\"\"\"\n",
        "    preds = autoencoder.predict(x)\n",
        "    preds_flat = preds.reshape((preds.shape[0], -1))\n",
        "    x_flat = x.reshape((x.shape[0], -1))\n",
        "    # ∞-norm per sample, then average\n",
        "    return np.mean(np.max(np.abs(preds_flat - x_flat), axis=1))\n",
        "\n",
        "def print_training_information(generator, classifier, X_test, iteration, autoencoder):\n",
        "    \"\"\"Print diagnostic metrics during training.\"\"\"\n",
        "    X_gen = generator.predict(X_test)\n",
        "    clf_test = classifier.predict(X_test)\n",
        "    clf_gen  = classifier.predict(X_gen)\n",
        "    delta_clf = (clf_gen - clf_test)[:, DESIRED_CLASS]\n",
        "\n",
        "    recon_error = np.mean(compute_reconstruction_error(X_gen, autoencoder))\n",
        "    l1_dist     = np.mean(np.abs(X_gen - X_test))\n",
        "\n",
        "    print('='*88)\n",
        "    print(f\"Iteration {iteration} @ {datetime.now()}\")\n",
        "    print(f\"Autoencoder reconstruction error (∞-norm): {recon_error:.3f}\")\n",
        "    print(f\"Counterfactual prediction gain (0→1): {delta_clf.mean():.3f}\")\n",
        "    print(f\"L1 distance (lower better): {l1_dist:.3f}\")\n",
        "    \n",
        "def format_metric(metric):\n",
        "    \"\"\"Return a formatted version of a metric, with the confidence interval.\"\"\"\n",
        "    return f\"{metric.mean():.3f} ± {1.96*metric.std()/np.sqrt(len(metric)):.3f}\"\n",
        "\n",
        "def compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None):\n",
        "    \"\"\" Summarize the relevant metrics in a dictionary. \"\"\"\n",
        "    reconstruction_error = compute_reconstruction_error(counterfactuals, autoencoder)\n",
        "    delta = np.abs(samples-counterfactuals)\n",
        "    l1_distances = delta.reshape(delta.shape[0], -1).sum(axis=1)\n",
        "    prediction_gain = (\n",
        "        classifier.predict(counterfactuals)[:, DESIRED_CLASS] - \n",
        "        classifier.predict(samples)[:, DESIRED_CLASS]\n",
        "    )\n",
        "\n",
        "    metrics = dict()\n",
        "    metrics[\"reconstruction_error\"] = format_metric(np.array([reconstruction_error]))\n",
        "    metrics[\"prediction_gain\"] = format_metric(prediction_gain)\n",
        "    metrics[\"sparsity\"] = format_metric(l1_distances)\n",
        "    metrics[\"latency\"] = format_metric(latencies)\n",
        "    batch_latency = batch_latency if batch_latency else sum(latencies)\n",
        "    metrics[\"latency_batch\"] = f\"{batch_latency:.3f}\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def generate_fake_samples(x, generator):\n",
        "    \"\"\"Use the input generator to generate samples.\"\"\"\n",
        "    return generator.predict(x)\n",
        "\n",
        "def data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Generate batches until exhaustion of the input data.\"\"\"\n",
        "    n_train = x.shape[0]\n",
        "    if y is not None:\n",
        "        assert n_train == len(y)\n",
        "    n_complete_batches, leftover = divmod(n_train, batch_size)\n",
        "    n_batches = n_complete_batches + bool(leftover)\n",
        "\n",
        "    perm = np.random.permutation(n_train)\n",
        "    for i in range(n_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        if y is not None:\n",
        "            output = (x[batch_idx], y[batch_idx])\n",
        "        else:\n",
        "            output = x[batch_idx]\n",
        "        yield output\n",
        "\n",
        "\n",
        "def infinite_data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Infinite batch generator.\"\"\"\n",
        "    batches = data_stream(x, y, batch_size=batch_size)\n",
        "    while True:\n",
        "        try:\n",
        "            yield next(batches)\n",
        "        except StopIteration:\n",
        "            batches = data_stream(x, y, batch_size=batch_size)\n",
        "            yield next(batches)\n",
        "\n",
        "def create_generator(in_shape=(X_train.shape[1],), residuals=True):\n",
        "    \"\"\"Define and compile the residual generator of the CounteRGAN.\"\"\"\n",
        "    generator_input = Input(shape=in_shape, name='generator_input')\n",
        "    generator = Dense(64, activation='relu')(generator_input)\n",
        "    generator = Dense(32, activation='relu')(generator)\n",
        "    generator = Dense(64, activation='relu')(generator)\n",
        "    generator = Dense(in_shape[0], activation='tanh')(generator)\n",
        "    generator_output = ActivityRegularization(l1=0., l2=1e-6)(generator)\n",
        "    \n",
        "    if residuals:\n",
        "        generator_output = Add(name=\"output\")([generator_input, generator_output])\n",
        "\n",
        "    return Model(inputs=generator_input, outputs=generator_output)\n",
        "\n",
        "\n",
        "def create_discriminator(in_shape=(X_train.shape[1],)):\n",
        "    \"\"\" Define a neural network binary classifier to classify real and generated \n",
        "    examples.\"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=in_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ], name=\"discriminator\")\n",
        "    optimizer = optimizers.legacy.Adam(learning_rate=0.0005, beta_1=0.5, decay=1e-8)\n",
        "    model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# def define_countergan(generator, discriminator, classifier, \n",
        "#                       input_shape=(X_train.shape[1],)):\n",
        "#     \"\"\"Combine a generator, discriminator, and fixed classifier into the CounteRGAN.\"\"\"\n",
        "#     discriminator.trainable = False\n",
        "#     classifier.trainable = False\n",
        "\n",
        "#     countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "#     x_generated = generator(countergan_input)\n",
        "\n",
        "#     countergan = Model(\n",
        "#         inputs=countergan_input, \n",
        "#         outputs=[discriminator(x_generated), classifier(x_generated)]\n",
        "#     )\n",
        "        \n",
        "#     optimizer = optimizers.legacy.RMSprop(learning_rate=2e-4, decay=1e-8)\n",
        "#     countergan.compile(optimizer, [\"binary_crossentropy\", \"categorical_crossentropy\"])\n",
        "#     return countergan\n",
        "\n",
        "def define_countergan(generator, discriminator, classifier, cost_weights, sparsity_lambda=0.1,immutable_idxs=None):\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable    = False\n",
        "    \"\"\"\n",
        "    Build the CounterGAN model that includes:\n",
        "      - discriminator loss\n",
        "      - classifier-target loss\n",
        "      - PLUS a sparsity (L1) penalty on feature changes weighted by cost_weights\n",
        "    \"\"\"\n",
        "    # inputs\n",
        "    x_input = Input(shape=(X_train.shape[1],), name=\"cf_input\")\n",
        "    # generate counterfactual\n",
        "    x_gen   = generator(x_input)\n",
        "    # discriminator output\n",
        "    y_disc  = discriminator(x_gen)\n",
        "    # classifier output (for your desired class)\n",
        "    y_clf   = classifier(x_gen)\n",
        "\n",
        "    # compute the L1 distance between gen and original\n",
        "    delta = x_gen - x_input  # shape: (batch, n_features)\n",
        "    # convert cost_weights dict to a tensor of shape (n_features,)\n",
        "    cost_vector = K.constant(\n",
        "        [cost_weights.get(f, 1.0) for f in features], dtype=\"float32\"\n",
        "    ) \n",
        "    # per-sample L1 penalty = sum(|Δxᵢ| * cost_vectorᵢ)\n",
        "    sparse_loss = K.sum(K.abs(delta) * cost_vector, axis=1)\n",
        "    # define the full model\n",
        "    cgan = tf.keras.models.Model(\n",
        "        inputs=x_input, outputs=[y_disc, y_clf], name=\"countergan\"\n",
        "    )\n",
        "    # add sparsity loss (scaled by lambda)\n",
        "    cgan.add_loss(sparsity_lambda * K.mean(sparse_loss))\n",
        "    # compile with existing GAN losses\n",
        "    cgan.compile(\n",
        "        optimizer=optimizers.legacy.RMSprop(learning_rate=2e-4, decay=1e-8),\n",
        "        loss=[\"binary_crossentropy\", \"categorical_crossentropy\"],\n",
        "        loss_weights=[1.0, 1.0]\n",
        "    )\n",
        "    return cgan\n",
        "\n",
        "def define_weighted_countergan(generator, discriminator, \n",
        "                               input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator and a discriminator for the weighted version of the \n",
        "    CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(inputs=countergan_input, outputs=discriminator(x_generated))\n",
        "    optimizer = optimizers.legacy.RMSprop(learning_rate=5e-4, decay=1e-8)\n",
        "    countergan.compile(optimizer, \"binary_crossentropy\")  \n",
        "    return countergan\n",
        "\n",
        "# def train_countergan(n_discriminator_steps, n_generator_steps, n_training_iterations,\n",
        "#                      classifier, discriminator, generator, batches, \n",
        "#                      weighted_version=False):\n",
        "#     \"\"\" Main function: train the CounteRGAN\"\"\"\n",
        "#     def check_divergence(x_generated):\n",
        "#         return np.all(np.isnan(x_generated))\n",
        "\n",
        "#     def print_training_information(generator, classifier, X_test, iteration):\n",
        "#         X_gen = generator.predict(X_test)\n",
        "#         clf_pred_test = classifier.predict(X_test)\n",
        "#         clf_pred = classifier.predict(X_gen)\n",
        "\n",
        "#         delta_clf_pred = (clf_pred - clf_pred_test)[:, DESIRED_CLASS]\n",
        "#         y_target = to_categorical([DESIRED_CLASS] * len(clf_pred), \n",
        "#                                   num_classes=N_CLASSES)\n",
        "#         print('='*88)\n",
        "#         print(f\"Training iteration {iteration} at {datetime.now()}\")\n",
        "        \n",
        "        \n",
        "#         reconstruction_error = np.mean(compute_reconstruction_error(X_gen, autoencoder))\n",
        "#         print(f\"Autoencoder reconstruction error (infinity to 0): {reconstruction_error:.3f}\")\n",
        "#         print(f\"Counterfactual prediction gain (0 to 1): {delta_clf_pred.mean():.3f}\")\n",
        "#         print(f\"Sparsity (L1, infinity to 0): {np.mean(np.abs(X_gen-X_test)):.3f}\")\n",
        "#     cw = cluster_profile_map[cluster_id][\"cost_weights\"]\n",
        "#     if weighted_version:\n",
        "#         countergan = define_weighted_countergan(generator, discriminator)\n",
        "#     else:\n",
        "        \n",
        "#         countergan = define_countergan(generator, discriminator, classifier,cost_weights=cw)\n",
        "\n",
        "#     for iteration in range(n_training_iterations):\n",
        "#         if iteration > 0:\n",
        "#             x_generated = generator.predict(x_fake_input)\n",
        "#             if check_divergence(x_generated):\n",
        "#                 print(\"Training diverged with the following loss functions:\")\n",
        "#                 print(discrim_loss_1, discrim_accuracy, gan_loss, \n",
        "#                     discrim_loss, discrim_loss_2, clf_loss)\n",
        "#                 break\n",
        "\n",
        "#         # Periodically print and plot training information \n",
        "#         if (iteration % 1000 == 0) or (iteration == n_training_iterations - 1):\n",
        "#             print_training_information(generator, classifier, X_test, iteration)\n",
        "\n",
        "#         # Train the discriminator\n",
        "#         discriminator.trainable = True\n",
        "#         for _ in range(n_discriminator_steps):\n",
        "#             x_fake_input, _ = next(batches)\n",
        "#             x_fake = generate_fake_samples(x_fake_input, generator)\n",
        "#             x_real = x_fake_input\n",
        "\n",
        "#             x_batch = np.concatenate([x_real, x_fake])\n",
        "#             y_batch = np.concatenate([np.ones(len(x_real)), np.zeros(len(x_fake))])\n",
        "            \n",
        "#             # Shuffle real and fake examples\n",
        "#             p = np.random.permutation(len(y_batch))\n",
        "#             x_batch, y_batch = x_batch[p], y_batch[p]\n",
        "\n",
        "#             if weighted_version:\n",
        "#                 classifier_scores = classifier.predict(x_batch)[:, DESIRED_CLASS]\n",
        "                \n",
        "#                 # The following update to the classifier scores is needed to have the \n",
        "#                 # same order of magnitude between real and generated samples losses\n",
        "#                 real_samples = np.where(y_batch == 1.)\n",
        "#                 average_score_real_samples = np.mean(classifier_scores[real_samples])\n",
        "#                 classifier_scores[real_samples] /= average_score_real_samples\n",
        "                \n",
        "#                 fake_samples = np.where(y_batch == 0.)\n",
        "#                 classifier_scores[fake_samples] = 1.\n",
        "\n",
        "#                 discriminator.train_on_batch(\n",
        "#                     x_batch, y_batch, sample_weight=classifier_scores\n",
        "#                 )\n",
        "#             else:\n",
        "#                 discriminator.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "#         # Train the generator \n",
        "#         discriminator.trainable = False\n",
        "#         for _ in range(n_generator_steps):\n",
        "#             x_fake_input, _ = next(batches)\n",
        "#             y_fake = np.ones(len(x_fake_input))\n",
        "#             if weighted_version:\n",
        "#                 countergan.train_on_batch(x_fake_input, y_fake)\n",
        "#             else:\n",
        "#                 y_target = to_categorical([DESIRED_CLASS] * len(x_fake_input), \n",
        "#                                           num_classes=N_CLASSES)\n",
        "#                 countergan.train_on_batch(x_fake_input, [y_fake, y_target])\n",
        "#     return countergan\n",
        "\n",
        "def train_countergan(n_discriminator_steps,\n",
        "                     n_generator_steps,\n",
        "                     n_training_iterations,\n",
        "                     classifier,\n",
        "                     discriminator,\n",
        "                     generator,\n",
        "                     countergan,\n",
        "                     batches,\n",
        "                     weighted_version=False):\n",
        "    \"\"\"\n",
        "    Train the CounterGAN model over provided batches.\n",
        "\n",
        "    countergan: a compiled Keras Model with two outputs:\n",
        "      [disc_output, class_output]\n",
        "    \"\"\"\n",
        "\n",
        "    def check_divergence(x_generated):\n",
        "        return np.all(np.isnan(x_generated))\n",
        "\n",
        "    for iteration in range(n_training_iterations):\n",
        "        # --- Discriminator updates ---\n",
        "        discriminator.trainable = True\n",
        "        for _ in range(n_discriminator_steps):\n",
        "            x_real, _ = next(batches)\n",
        "            x_fake = generator.predict(x_real)\n",
        "\n",
        "            X_disc = np.vstack([x_real, x_fake])\n",
        "            y_disc = np.concatenate([np.ones(len(x_real)), np.zeros(len(x_fake))])\n",
        "\n",
        "            # shuffle\n",
        "            idx = np.random.permutation(len(y_disc))\n",
        "            X_disc, y_disc = X_disc[idx], y_disc[idx]\n",
        "\n",
        "            if weighted_version:\n",
        "                # weight real examples by classifier confidence\n",
        "                clf_scores = classifier.predict(X_disc)[:, DESIRED_CLASS]\n",
        "                real_mask = (y_disc == 1)\n",
        "                avg_real = np.mean(clf_scores[real_mask]) + 1e-8\n",
        "                clf_scores[real_mask] /= avg_real\n",
        "                clf_scores[~real_mask] = 1.0\n",
        "                discriminator.train_on_batch(X_disc, y_disc, sample_weight=clf_scores)\n",
        "            else:\n",
        "                discriminator.train_on_batch(X_disc, y_disc)\n",
        "\n",
        "        # --- Generator (CounterGAN) updates ---\n",
        "        discriminator.trainable = False\n",
        "        for _ in range(n_generator_steps):\n",
        "            x_in, _ = next(batches)\n",
        "            # discriminator target: want them all judged as real\n",
        "            y_fake = np.ones(len(x_in))\n",
        "            # classifier target: push to desired class\n",
        "            y_target = to_categorical([DESIRED_CLASS] * len(x_in), num_classes=N_CLASSES)\n",
        "\n",
        "            if weighted_version:\n",
        "                # reuse clf_scores to weight the combined update\n",
        "                clf_scores = classifier.predict(x_in)[:, DESIRED_CLASS]\n",
        "                # no need to normalize here; give equal weight\n",
        "                countergan.train_on_batch(\n",
        "                  x_in,\n",
        "                  [y_fake, y_target],\n",
        "                  sample_weight=clf_scores\n",
        "                )\n",
        "            else:\n",
        "                countergan.train_on_batch(x_in, [y_fake, y_target])\n",
        "\n",
        "        # optional divergence check\n",
        "        if (iteration % 1000 == 0) or (iteration == n_training_iterations - 1):\n",
        "            x_gen = generator.predict(x_real)\n",
        "            if check_divergence(x_gen):\n",
        "                print(f\"⚠️  Divergence at iteration {iteration}, stopping early\")\n",
        "                break\n",
        "\n",
        "    return countergan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bebe8b0b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "— linkage = average —\n",
            " k= 2 → sizes={0: 996, 1: 3}  |  sil=0.166\n",
            " k= 3 → sizes={0: 990, 1: 3, 2: 6}  |  sil=0.120\n",
            " k= 4 → sizes={0: 980, 1: 10, 2: 6, 3: 3}  |  sil=0.082\n",
            " k= 5 → sizes={0: 965, 1: 15, 2: 6, 3: 3, 4: 10}  |  sil=0.058\n",
            " k= 6 → sizes={0: 961, 1: 15, 2: 6, 3: 3, 4: 10, 5: 4}  |  sil=0.033\n",
            "\n",
            "— linkage = complete —\n",
            " k= 2 → sizes={0: 854, 1: 145}  |  sil=0.120\n",
            " k= 3 → sizes={0: 485, 1: 145, 2: 369}  |  sil=0.019\n",
            " k= 4 → sizes={0: 369, 1: 145, 2: 288, 3: 197}  |  sil=0.028\n",
            " k= 5 → sizes={0: 145, 1: 197, 2: 288, 3: 217, 4: 152}  |  sil=0.021\n",
            " k= 6 → sizes={0: 288, 1: 197, 2: 80, 3: 217, 4: 152, 5: 65}  |  sil=0.018\n",
            "\n",
            "— linkage = single —\n",
            " k= 2 → sizes={0: 998, 1: 1}  |  sil=0.223\n",
            " k= 3 → sizes={0: 997, 1: 1, 2: 1}  |  sil=0.160\n",
            " k= 4 → sizes={0: 996, 1: 1, 2: 1, 3: 1}  |  sil=0.103\n",
            " k= 5 → sizes={0: 995, 1: 1, 2: 1, 3: 1, 4: 1}  |  sil=0.068\n",
            " k= 6 → sizes={0: 994, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}  |  sil=0.041\n"
          ]
        }
      ],
      "source": [
        "import gower\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# --- 0) raw_df: your un-encoded DataFrame of 37 columns, no target ---\n",
        "raw_df = df_pre_encoding.drop(columns=['Outcome'])  # drop target column if present\n",
        "# merge X_train and X_test\n",
        "# X_train = np.vstack([X_train, X_test])\n",
        "D = gower.gower_matrix(raw_df)\n",
        "\n",
        "\n",
        "for linkage in [\"average\",\"complete\",\"single\"]:\n",
        "    print(f\"\\n— linkage = {linkage} —\")\n",
        "    for k in range(2, 7):\n",
        "        model  = AgglomerativeClustering(\n",
        "                    n_clusters=k,\n",
        "                    metric=\"precomputed\",\n",
        "                    linkage=linkage\n",
        "                )\n",
        "        labels = model.fit_predict(D)  # your Gower matrix\n",
        "        sizes  = pd.Series(labels).value_counts().sort_index()\n",
        "        sil    = silhouette_score(D, labels, metric=\"precomputed\")\n",
        "        print(f\" k={k:>2} → sizes={sizes.to_dict()}  |  sil={sil:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd2d5be",
      "metadata": {},
      "source": [
        "Given your balance + silhouette trade-offs, complete linkage with k=4k=4k=4 is the sweet spot:\n",
        "•\tCluster sizes\n",
        "{369,  145,  288,  197} (all > 20)\n",
        "•\tSilhouette\n",
        "0.028 on Gower distances (only slightly below the peak at k=2)\n",
        "That gives you four reasonably large, interpretable segments—exactly in your 3–4 cluster sweet spot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a024ca3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chosen k = 4\n",
            " Cluster sizes: {0: 369, 1: 145, 2: 288, 3: 197}\n",
            " Silhouette (Gower) = 0.028\n",
            "\n",
            "          Month  Credit_Amount  Installment_Rate  Residence_Duration     Age\n",
            "Cluster                                                                     \n",
            "0        18.859       2728.309             3.005               2.694  34.859\n",
            "1        26.745       4861.779             3.110               3.338  41.145\n",
            "2        19.274       2855.771             2.712               2.694  31.694\n",
            "3        22.888       3735.650             3.188               2.980  38.183\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import gower\n",
        "\n",
        "# 1) raw_df: your pre-encoding DataFrame of 37 mixed‐type features\n",
        "D = gower.gower_matrix(raw_df)\n",
        "\n",
        "# 2) Fit with complete linkage and k=4\n",
        "best_k = 4\n",
        "model = AgglomerativeClustering(\n",
        "    n_clusters=best_k,\n",
        "    metric=\"precomputed\",\n",
        "    linkage=\"complete\"\n",
        ")\n",
        "labels = model.fit_predict(D)\n",
        "\n",
        "# 3) Confirm sizes & silhouette\n",
        "sizes = pd.Series(labels).value_counts().sort_index()\n",
        "sil   = silhouette_score(D, labels, metric=\"precomputed\")\n",
        "print(f\"\\nChosen k = {best_k}\")\n",
        "print(\" Cluster sizes:\", sizes.to_dict())\n",
        "print(f\" Silhouette (Gower) = {sil:.3f}\\n\")\n",
        "\n",
        "# 4) (Optionally) inspect cluster means on raw_df for labeling\n",
        "raw_df['Cluster'] = labels\n",
        "# First identify numeric columns\n",
        "numeric_cols = raw_df.select_dtypes(include=['number']).columns\n",
        "# Then compute means\n",
        "print(raw_df.groupby('Cluster')[numeric_cols].mean().iloc[:, :5].round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "599dee00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "  Immutable: ['Age', 'Foreign_Worker', 'Other_Debtors', 'Other_Installment_Plans']\n",
            "  Actionable: ['Credit_Amount', 'Credit_History', 'Employment', 'Existing_Credits', 'Housing'] …(+ 11 more)\n",
            "\n",
            "Cluster 1:\n",
            "  Immutable: ['Age', 'Foreign_Worker', 'Other_Debtors']\n",
            "  Actionable: ['Credit_Amount', 'Credit_History', 'Employment', 'Existing_Credits', 'Housing'] …(+ 12 more)\n",
            "\n",
            "Cluster 2:\n",
            "  Immutable: ['Age', 'Foreign_Worker']\n",
            "  Actionable: ['Credit_Amount', 'Credit_History', 'Employment', 'Existing_Credits', 'Housing'] …(+ 13 more)\n",
            "\n",
            "Cluster 3:\n",
            "  Immutable: ['Age', 'Foreign_Worker', 'Housing', 'Other_Debtors']\n",
            "  Actionable: ['Credit_Amount', 'Credit_History', 'Employment', 'Existing_Credits', 'Installment_Rate'] …(+ 11 more)\n",
            "\n",
            "[{\"cluster_id\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/reduce\": [{\"py/type\": \"numpy.dtype\"}, {\"py/tuple\": [\"i8\", false, true]}, {\"py/tuple\": [3, \"<\", null, null, null, -1, -1, 0]}]}, {\"py/b64\": \"AAAAAAAAAAA=\"}]}]}, \"immutable_features\": [\"Age\", \"Foreign_Worker\", \"Other_Debtors\", \"Other_Installment_Plans\"], \"actionable_features\": [\"Credit_Amount\", \"Credit_History\", \"Employment\", \"Existing_Credits\", \"Housing\", \"Installment_Rate\", \"Job\", \"Month\", \"Num_Liable_People\", \"Personal_Status\", \"Property\", \"Purpose\", \"Residence_Duration\", \"Savings\", \"Status\", \"Telephone\"], \"cost_weights\": {\"Credit_Amount\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/reduce\": [{\"py/type\": \"numpy.dtype\"}, {\"py/tuple\": [\"f8\", false, true]}, {\"py/tuple\": [3, \"<\", null, null, null, -1, -1, 0]}]}, {\"py/b64\": \"PuJYOfxZLD8=\"}]}]}, \"Credit_History\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"zXCZEdHS8D8=\"}]}]}, \"Employment\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"qpNYP9W06j8=\"}]}]}, \"Existing_Credits\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"zQfLQVVp6j8=\"}]}]}, \"Housing\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"WzXRH+DnBkA=\"}]}]}, \"Installment_Rate\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"AMVNH4mW3T8=\"}]}]}, \"Job\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"Ps4YCpUx/T8=\"}]}]}, \"Month\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"FaccX6bQpz8=\"}]}]}, \"Num_Liable_People\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"klRmy8xZ9z8=\"}]}]}, \"Personal_Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"eCgzbn9c8D8=\"}]}]}, \"Property\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"DVJTZrTy6j8=\"}]}]}, \"Purpose\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"UQw4O2iP6T8=\"}]}]}, \"Residence_Duration\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"lniUDBUh3j8=\"}]}]}, \"Savings\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"XF6f+c/S9D8=\"}]}]}, \"Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"VPlSPRHH7T8=\"}]}]}, \"Telephone\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"30Ges8FB9D8=\"}]}]}}}, {\"cluster_id\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 3}, {\"py/b64\": \"AQAAAAAAAAA=\"}]}]}, \"immutable_features\": [\"Age\", \"Foreign_Worker\", \"Other_Debtors\"], \"actionable_features\": [\"Credit_Amount\", \"Credit_History\", \"Employment\", \"Existing_Credits\", \"Housing\", \"Installment_Rate\", \"Job\", \"Month\", \"Num_Liable_People\", \"Other_Installment_Plans\", \"Personal_Status\", \"Property\", \"Purpose\", \"Residence_Duration\", \"Savings\", \"Status\", \"Telephone\"], \"cost_weights\": {\"Credit_Amount\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"6GG12fQTIT8=\"}]}]}, \"Credit_History\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"/Xhj9Trg6j8=\"}]}]}, \"Employment\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"Be6xWDUw6D8=\"}]}]}, \"Existing_Credits\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"EYKp/fa06D8=\"}]}]}, \"Housing\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"KYoG6DRQ+T8=\"}]}]}, \"Installment_Rate\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"JDs4yRR33D8=\"}]}]}, \"Job\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"MRfaLOm+8D8=\"}]}]}, \"Month\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"3SRQoj78oD8=\"}]}]}, \"Num_Liable_People\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"13OY2dQC8T8=\"}]}]}, \"Other_Installment_Plans\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"110zLVCM+j8=\"}]}]}, \"Personal_Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"nvPPLSQkAkA=\"}]}]}, \"Property\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"0ngoiEjEAkA=\"}]}]}, \"Purpose\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"ysf5oWZY5D8=\"}]}]}, \"Residence_Duration\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"HYxHCH7+3D8=\"}]}]}, \"Savings\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"uHMybleO8T8=\"}]}]}, \"Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"2Fa1GoLq6T8=\"}]}]}, \"Telephone\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"6fvfvJQY8z8=\"}]}]}}}, {\"cluster_id\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 3}, {\"py/b64\": \"AgAAAAAAAAA=\"}]}]}, \"immutable_features\": [\"Age\", \"Foreign_Worker\"], \"actionable_features\": [\"Credit_Amount\", \"Credit_History\", \"Employment\", \"Existing_Credits\", \"Housing\", \"Installment_Rate\", \"Job\", \"Month\", \"Num_Liable_People\", \"Other_Debtors\", \"Other_Installment_Plans\", \"Personal_Status\", \"Property\", \"Purpose\", \"Residence_Duration\", \"Savings\", \"Status\", \"Telephone\"], \"cost_weights\": {\"Credit_Amount\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"1jX8ZrMYJD8=\"}]}]}, \"Credit_History\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"rDknf2w/9D8=\"}]}]}, \"Employment\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"uJPMHvq35D8=\"}]}]}, \"Existing_Credits\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"ZdC6jVgR7D8=\"}]}]}, \"Housing\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"ClfyL31e8j8=\"}]}]}, \"Installment_Rate\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"847Rn3W61D8=\"}]}]}, \"Job\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"EoSf6Pkh8T8=\"}]}]}, \"Month\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"I6LNVaxyoT8=\"}]}]}, \"Num_Liable_People\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"lGN0CQyN9T8=\"}]}]}, \"Other_Debtors\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"54X89/DXBkA=\"}]}]}, \"Other_Installment_Plans\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"dcm9IFRI/D8=\"}]}]}, \"Personal_Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"z8eDzfcz6j8=\"}]}]}, \"Property\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"aQtXaVuo4j8=\"}]}]}, \"Purpose\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"Xe5JS7q64T8=\"}]}]}, \"Residence_Duration\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"Ba34KPRO1T8=\"}]}]}, \"Savings\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"9VtnNPCN9j8=\"}]}]}, \"Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"XjfxIlwi5j8=\"}]}]}, \"Telephone\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"97k0q30uAkA=\"}]}]}}}, {\"cluster_id\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 3}, {\"py/b64\": \"AwAAAAAAAAA=\"}]}]}, \"immutable_features\": [\"Age\", \"Foreign_Worker\", \"Housing\", \"Other_Debtors\"], \"actionable_features\": [\"Credit_Amount\", \"Credit_History\", \"Employment\", \"Existing_Credits\", \"Installment_Rate\", \"Job\", \"Month\", \"Num_Liable_People\", \"Other_Installment_Plans\", \"Personal_Status\", \"Property\", \"Purpose\", \"Residence_Duration\", \"Savings\", \"Status\", \"Telephone\"], \"cost_weights\": {\"Credit_Amount\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"yOM2bQqDJj8=\"}]}]}, \"Credit_History\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"IQ9cZls16z8=\"}]}]}, \"Employment\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"fOW4tdyt7D8=\"}]}]}, \"Existing_Credits\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"jYPxh7MQ6z8=\"}]}]}, \"Installment_Rate\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"beeFsRE43z8=\"}]}]}, \"Job\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"MclcO+QT8D8=\"}]}]}, \"Month\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"1I6IyC6ipT8=\"}]}]}, \"Num_Liable_People\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"BOhxescI9D8=\"}]}]}, \"Other_Installment_Plans\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"gvIYEYGUAEA=\"}]}]}, \"Personal_Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"C2dv69vyAkA=\"}]}]}, \"Property\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"bHrrS4OU8D8=\"}]}]}, \"Purpose\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"3iM6QBBr5j8=\"}]}]}, \"Residence_Duration\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"4FZl368U4D8=\"}]}]}, \"Savings\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"0q8VV09s7D8=\"}]}]}, \"Status\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"0BVTEExs/D8=\"}]}]}, \"Telephone\": {\"py/reduce\": [{\"py/function\": \"numpy.core.multiarray.scalar\"}, {\"py/tuple\": [{\"py/id\": 8}, {\"py/b64\": \"DBKLbANo9D8=\"}]}]}}}]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# thresholds—tweak to taste\n",
        "NUMERIC_STD_THR   = 0.05    # very low spread → almost constant\n",
        "CATEGORICAL_PCT_THR = 0.90  # mode covers 90%+ → nearly unanimous\n",
        "\n",
        "# separate cols\n",
        "num_cols = raw_df.select_dtypes(include='number').columns.drop('Cluster')\n",
        "cat_cols = raw_df.select_dtypes(include=['object','category']).columns\n",
        "\n",
        "# precompute stats per cluster\n",
        "grp = raw_df.groupby('Cluster')\n",
        "\n",
        "# numeric: std per cluster\n",
        "num_std = grp[num_cols].std()\n",
        "\n",
        "# categorical: relative freq of the mode per cluster\n",
        "def mode_pct(s):\n",
        "    fq = s.value_counts(normalize=True)\n",
        "    return fq.iloc[0]  if not fq.empty else 0\n",
        "cat_mode_pct = grp[cat_cols].agg(mode_pct)\n",
        "\n",
        "final_profiles = []\n",
        "for cid in sorted(raw_df['Cluster'].unique()):\n",
        "    imm = []\n",
        "    # collect numeric‐immutables\n",
        "    for f in num_cols:\n",
        "        if num_std.loc[cid, f] < NUMERIC_STD_THR:\n",
        "            imm.append(f)\n",
        "    # collect categorical‐immutables\n",
        "    for f in cat_cols:\n",
        "        if cat_mode_pct.loc[cid, f] >= CATEGORICAL_PCT_THR:\n",
        "            imm.append(f)\n",
        "\n",
        "    # always include truly immutable by domain\n",
        "    for global_imm in [\"Age\", \"Foreign_Worker\"]:  \n",
        "        if global_imm not in imm:\n",
        "            imm.append(global_imm)\n",
        "\n",
        "    # actionable = all remaining features\n",
        "    all_feats = num_cols.tolist() + cat_cols.tolist()\n",
        "    act  = [f for f in all_feats if f not in imm]\n",
        "\n",
        "    final_profiles.append({\n",
        "      \"cluster_id\": cid,\n",
        "      \"immutable_features\": sorted(imm),\n",
        "      \"actionable_features\": sorted(act)\n",
        "    })\n",
        "\n",
        "# peek\n",
        "for p in final_profiles:\n",
        "    print(f\"Cluster {p['cluster_id']}:\")\n",
        "    print(\"  Immutable:\", p['immutable_features'])\n",
        "    print(\"  Actionable:\", p['actionable_features'][:5], \"…(+\", \n",
        "          len(p['actionable_features'])-5, \"more)\\n\")\n",
        "\n",
        "\n",
        "# def remove_circular(obj, seen=None):\n",
        "\n",
        "\n",
        "# --- assume these are already computed from your raw_df + labels ---\n",
        "# num_std:        DataFrame (clusters × numeric_cols) of within-cluster standard deviations\n",
        "# cat_mode_pct:   DataFrame (clusters × cat_cols) of within-cluster mode frequency\n",
        "# final_profiles: a list of dicts for each cluster, each with keys\n",
        "#                 \"cluster_id\", \"immutable_features\", \"actionable_features\"\n",
        "\n",
        "# thresholds\n",
        "epsilon = 1e-6\n",
        "\n",
        "# Build cost_weights for each profile\n",
        "for prof in final_profiles:\n",
        "    cid    = prof[\"cluster_id\"]\n",
        "    action = prof[\"actionable_features\"]\n",
        "    raw_w  = {}\n",
        "    # numeric features: cost ∝ 1 / std   (harder to move → higher cost)\n",
        "    for f in action:\n",
        "        if f in num_std.columns:\n",
        "            std = num_std.loc[cid, f]\n",
        "            raw_w[f] = 1.0 / (std + epsilon)\n",
        "        else:\n",
        "            # categorical features: cost ∝ 1 / (1 - mode_pct)\n",
        "            pct = cat_mode_pct.loc[cid, f]\n",
        "            raw_w[f] = 1.0 / ((1.0 - pct) + epsilon)\n",
        "\n",
        "    # normalize so average weight = 1.0 (sum weights = number of actionables)\n",
        "    total = sum(raw_w.values())\n",
        "    n_act = len(raw_w)\n",
        "    cost_w = { f: (w * n_act / total) for f, w in raw_w.items() }\n",
        "\n",
        "    prof[\"cost_weights\"] = cost_w\n",
        "\n",
        "# now peek at one\n",
        "\n",
        "print(jsonpickle.encode(final_profiles))\n",
        "\n",
        "\n",
        "\n",
        "clean_profiles = []\n",
        "for prof in final_profiles:\n",
        "    # Extract just built‐in types\n",
        "    clean = {\n",
        "        \"cluster_id\": prof[\"cluster_id\"],\n",
        "        \"immutable_features\": prof[\"immutable_features\"],\n",
        "        \"actionable_features\": prof[\"actionable_features\"],\n",
        "        \"cost_weights\": {f: float(w) for f, w in prof[\"cost_weights\"].items()}\n",
        "    }\n",
        "    clean_profiles.append(clean)\n",
        "\n",
        "with open(\"final_user_profiles_with_metadata.json\", \"w\") as f:\n",
        "    json.dump(clean_profiles, f, indent=2, default=lambda obj: obj.item() if hasattr(obj, 'item') else str(obj))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "65d2998c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Numeric feature means by cluster ===\n",
            "          Month  Credit_Amount  Installment_Rate  Residence_Duration     Age  \\\n",
            "Cluster                                                                        \n",
            "0        18.859       2728.309             3.005               2.694  34.859   \n",
            "1        26.745       4861.779             3.110               3.338  41.145   \n",
            "2        19.274       2855.771             2.712               2.694  31.694   \n",
            "3        22.888       3735.650             3.188               2.980  38.183   \n",
            "\n",
            "         Existing_Credits  Num_Liable_People  \n",
            "Cluster                                       \n",
            "0                   1.453              1.138  \n",
            "1                   1.414              1.262  \n",
            "2                   1.222              1.090  \n",
            "3                   1.584              1.203  \n",
            "\n",
            "=== Categorical feature modes by cluster ===\n",
            "                      Status                            Credit_History  \\\n",
            "Cluster                                                                  \n",
            "0        no checking account  existing credits paid back duly till now   \n",
            "1          0 <= ... < 200 DM  existing credits paid back duly till now   \n",
            "2                     < 0 DM  existing credits paid back duly till now   \n",
            "3        no checking account   critical account/other credits existing   \n",
            "\n",
            "        Purpose   Savings          Employment  \\\n",
            "Cluster                                         \n",
            "0           A43  < 100 DM  1 <= ... < 4 years   \n",
            "1           A40  < 100 DM          >= 7 years   \n",
            "2           A42  < 100 DM  1 <= ... < 4 years   \n",
            "3           A43  < 100 DM          >= 7 years   \n",
            "\n",
            "                            Personal_Status Other_Debtors  \\\n",
            "Cluster                                                     \n",
            "0                              male: single          none   \n",
            "1                              male: single          none   \n",
            "2        female: divorced/separated/married          none   \n",
            "3                              male: single          none   \n",
            "\n",
            "                                                  Property  \\\n",
            "Cluster                                                      \n",
            "0                                              real estate   \n",
            "1                                      unknown/no property   \n",
            "2        building society savings agreement/life insurance   \n",
            "3                         car or other, not in attribute 6   \n",
            "\n",
            "        Other_Installment_Plans   Housing   Job  \\\n",
            "Cluster                                           \n",
            "0                          none       own  A173   \n",
            "1                          none  for free  A173   \n",
            "2                          none       own  A173   \n",
            "3                          none       own  A173   \n",
            "\n",
            "                                         Telephone Foreign_Worker  \n",
            "Cluster                                                            \n",
            "0                                             none            yes  \n",
            "1        yes, registered under the customer's name            yes  \n",
            "2                                             none            yes  \n",
            "3        yes, registered under the customer's name            yes  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# # 1) Numeric summary\n",
        "# num_cols = raw_df.select_dtypes(include='number').columns.drop('Cluster')\n",
        "# cluster_num_means = (\n",
        "#     raw_df\n",
        "#     .groupby('Cluster')[num_cols]\n",
        "#     .mean()\n",
        "#     .round(3)\n",
        "# )\n",
        "\n",
        "# # 2) Categorical summary (mode)\n",
        "# cat_cols = raw_df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# def mode_func(series):\n",
        "#     m = series.mode()\n",
        "#     return m.iloc[0] if not m.empty else None\n",
        "\n",
        "# cluster_cat_modes = (\n",
        "#     raw_df\n",
        "#     .groupby('Cluster')[cat_cols]\n",
        "#     .agg(mode_func)\n",
        "# )\n",
        "\n",
        "# # 3) Display\n",
        "# print(\"=== Numeric feature means by cluster ===\")\n",
        "# print(cluster_num_means)\n",
        "\n",
        "# print(\"\\n=== Categorical feature modes by cluster ===\")\n",
        "# print(cluster_cat_modes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1fc019d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0: credit≈2728, age≈35, housing=own, purpose=A43, …\n",
            "Cluster 1: credit≈4862, age≈41, housing=for free, purpose=A40, …\n",
            "Cluster 2: credit≈2856, age≈32, housing=own, purpose=A42, …\n",
            "Cluster 3: credit≈3736, age≈38, housing=own, purpose=A43, …\n"
          ]
        }
      ],
      "source": [
        "# # 1. Summarize each cluster’s characteristics\n",
        "# # Use the numeric means and categorical modes you just computed to write a 1–2 line “label” for each cluster.\n",
        "\n",
        "# # numeric_means, categorical_modes from the last step\n",
        "# for cid in range(4):\n",
        "#     num = cluster_num_means.loc[cid]\n",
        "#     cat = cluster_cat_modes.loc[cid]\n",
        "#     # e.g. pick Purpose, Housing, Employment level from `cat`,\n",
        "#     # Credit_Amount bucket and Age group from `num`, etc.\n",
        "#     print(f\"Cluster {cid}: credit≈{num.Credit_Amount:.0f}, age≈{num.Age:.0f}, \"\n",
        "#           f\"housing={cat.Housing}, purpose={cat.Purpose}, …\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "21bd7efa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your JSON list of profiles\n",
        "with open(\"final_user_profiles_with_metadata.json\") as f:\n",
        "    profiles = json.load(f)\n",
        "\n",
        "for prof in profiles:\n",
        "    # freeze‐list of encoded indices\n",
        "    prof[\"freeze_idxs\"] = []\n",
        "    for raw in prof[\"immutable_features\"]:\n",
        "        prof[\"freeze_idxs\"].extend(raw_to_encoded[raw])\n",
        "\n",
        "    # build an encoded‐cost vector of length = n_encoded_cols\n",
        "    cw = np.ones(len(encoded_cols), dtype=float)\n",
        "    for raw, w in prof[\"cost_weights\"].items():\n",
        "        for idx in raw_to_encoded[raw]:\n",
        "            cw[idx] = w\n",
        "    prof[\"cost_vector\"] = cw\n",
        "\n",
        "    # sparsity λ is already prof[\"sparsity_lambda\"]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Load Final Profile JSON ---\n",
        "with open('final_user_profiles_with_metadata.json', 'r') as f:\n",
        "    cluster_profiles = json.load(f)\n",
        "\n",
        "# Build a lookup map\n",
        "cluster_profile_map = {p['cluster_id']: p for p in cluster_profiles}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Clustering ---\n",
        "k_fixed = len(cluster_profiles)\n",
        "important_features = [33, 6, 28, 3, 23, 14, 4] # Mix immutable & actionable with High explanatory power These features had the highest SHAP/mutual-information scores in a quick feature-importance pass. Also They work with Euclidean linkage since they are Numeric/ordinal.\n",
        "\n",
        "X_train_subset = X_train[:, important_features]\n",
        "cluster_model = AgglomerativeClustering(n_clusters=k_fixed)\n",
        "main_cluster_labels = cluster_model.fit_predict(X_train_subset)\n",
        "print(f\"Fixed number of clusters: {k_fixed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "1/1 [==============================] - 0s 61ms/step\n"
          ]
        }
      ],
      "source": [
        "# --- Assign Test Samples to Clusters ---\n",
        "centroid_model = NearestCentroid()\n",
        "X_merge = np.vstack([X_train, X_test])\n",
        "centroid_model.fit(X_merge, labels)\n",
        "test_labels = centroid_model.predict(X_test)\n",
        "\n",
        "results = []\n",
        "for i, cid in enumerate(X_test):\n",
        "    prof = profiles[test_labels[i]]\n",
        "    gen  = load_model(f\"generators/cluster_{prof['cluster_id']}.h5\")\n",
        "\n",
        "    cf = gen.predict(X_test[i:i+1])  # generate counterfactual for this sample\n",
        "    cf = cf[0]  # remove batch dimension    \n",
        "    \n",
        "\n",
        "    # # enforce immutables\n",
        "    # cf[prof[\"freeze_idxs\"]] = X_test[prof[\"freeze_idxs\"]]\n",
        "\n",
        "    # evaluate / save\n",
        "    results.append(cf)\n",
        "    \n",
        "def np_encoder(obj):\n",
        "    import numpy as np\n",
        "    if isinstance(obj, np.generic):\n",
        "        return obj.item()\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
        "\n",
        "with open(\"your_output_file.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2, default=np_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Prepare Cluster-Specific Training Data ---\n",
        "cluster_data = defaultdict(lambda: {'X': [], 'y': []})\n",
        "for x, y, cid in zip(X_train, y_train, main_cluster_labels):\n",
        "    cluster_data[cid]['X'].append(x)\n",
        "    cluster_data[cid]['y'].append(y)\n",
        "for cid in cluster_data:\n",
        "    cluster_data[cid]['X'] = np.vstack(cluster_data[cid]['X'])\n",
        "    cluster_data[cid]['y'] = np.array(cluster_data[cid]['y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_all_cluster_gans(cluster_profiles,\n",
        "                           cluster_data,\n",
        "                           classifier,\n",
        "                           output_dir='generators',\n",
        "                           n_iterations=300):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for prof in cluster_profiles:\n",
        "        cid = prof['cluster_id']\n",
        "        cw  = prof['cost_weights']\n",
        "        lam = prof['sparsity_lambda']\n",
        "        imm = prof['freeze_idxs']  # if you need it for weighted_version\n",
        "\n",
        "        print(f\"\\n[Cluster {cid}] cost_weights = {cw}, sparsity_lambda = {lam}\")\n",
        "\n",
        "        # 1) Build fresh generator & discriminator\n",
        "        gen  = create_generator(residuals=True)\n",
        "        disc = create_discriminator()\n",
        "\n",
        "        # 2) Define your CounterGAN (with sparsity loss)\n",
        "        #    Pass cw dict and lambda into your define_countergan\n",
        "        countergan = define_countergan(\n",
        "            generator=gen,\n",
        "            discriminator=disc,\n",
        "            classifier=classifier,\n",
        "            cost_weights=cw,\n",
        "            sparsity_lambda= lam,\n",
        "            immutable_idxs= imm  \n",
        "        )\n",
        "\n",
        "        # 3) Prepare the data stream\n",
        "        Xc, yc = cluster_data[cid]['X'], cluster_data[cid]['y']\n",
        "        batches = infinite_data_stream(Xc, yc, batch_size=128)\n",
        "\n",
        "        # 4) Train\n",
        "        t0 = time.time()\n",
        "        # Note: train_countergan must now accept a prebuilt countergan model\n",
        "        train_countergan(\n",
        "            n_discriminator_steps=2,\n",
        "            n_generator_steps=4,\n",
        "            n_training_iterations=n_iterations,\n",
        "            classifier=classifier,\n",
        "            discriminator=disc,\n",
        "            generator=gen,\n",
        "            countergan=countergan,          # <-- pass it here\n",
        "            batches=batches,\n",
        "            weighted_version=True,\n",
        "            # immutable_features=imm         # optionally if you do sample‐weighting\n",
        "        )\n",
        "        print(f\"[Cluster {cid}] trained in {time.time()-t0:.1f}s\")\n",
        "\n",
        "        # 5) Save the generator for inference\n",
        "        gen.save(f\"{output_dir}/cluster_{cid}.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "97138500",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cluster_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Train all personalized GANs per cluster ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_all_cluster_gans(\n\u001b[1;32m      3\u001b[0m     cluster_profiles\u001b[38;5;241m=\u001b[39mprof,  \n\u001b[0;32m----> 4\u001b[0m     cluster_data\u001b[38;5;241m=\u001b[39m\u001b[43mcluster_data\u001b[49m,\n\u001b[1;32m      5\u001b[0m     classifier\u001b[38;5;241m=\u001b[39mclassifier,\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerators\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     n_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cluster_data' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Train all personalized GANs per cluster ---\n",
        "train_all_cluster_gans(\n",
        "    cluster_profiles=prof,  \n",
        "    cluster_data=cluster_data,\n",
        "    classifier=classifier,\n",
        "    output_dir='generators',\n",
        "    n_iterations=300\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Generate Personalized Counterfactuals ---\n",
        "generators = {cid: load_model(f\"generators/cluster_{cid}.h5\")\n",
        "              for cid in cluster_profile_map}\n",
        "\n",
        "counterfactuals = []\n",
        "personalized_latencies = []\n",
        "\n",
        "for i, x0 in enumerate(X_test):\n",
        "    cid = test_profiles[i]['cluster_id']\n",
        "    gen = generators[cid]\n",
        "    t0 = time.time()\n",
        "    cf = gen.predict(x0[None])[0].astype(np.float32)\n",
        "    personalized_latencies.append(1000*(time.time()-t0))\n",
        "    counterfactuals.append(cf)\n",
        "\n",
        "counterfactuals = np.stack(counterfactuals, axis=0)\n",
        "latencies      = np.array(personalized_latencies, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Overall Evaluation ---\n",
        "from pprint import pprint\n",
        "\n",
        "print(\"\\nPersonalized GAN evaluation:\")\n",
        "personalized_metrics = compute_metrics(\n",
        "    samples=X_test,\n",
        "    counterfactuals=counterfactuals,\n",
        "    latencies=latencies,\n",
        "    classifier=classifier,\n",
        "    autoencoder=autoencoder\n",
        ")\n",
        "pprint(personalized_metrics)\n",
        "\n",
        "print(\"\\nBaseline CounterGAN evaluation:\")\n",
        "baseline_cfs = generators[0].predict(X_test)\n",
        "baseline_metrics = compute_metrics(\n",
        "    samples=X_test,\n",
        "    counterfactuals=baseline_cfs,\n",
        "    latencies=np.zeros(len(X_test)),\n",
        "    classifier=classifier,\n",
        "    autoencoder=autoencoder\n",
        ")\n",
        "pprint(baseline_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Per-Cluster Metrics & Plots ---\n",
        "cluster_metrics = []\n",
        "for cid in np.unique(main_cluster_labels):\n",
        "    idxs = [i for i,tp in enumerate(test_profiles) if tp['cluster_id']==cid]\n",
        "    Xc_te = X_test[idxs]\n",
        "    cf_c  = counterfactuals[idxs]\n",
        "    lat_c = latencies[idxs]\n",
        "\n",
        "    m = compute_metrics(\n",
        "        samples=Xc_te,\n",
        "        counterfactuals=cf_c,\n",
        "        latencies=lat_c,\n",
        "        classifier=classifier,\n",
        "        autoencoder=autoencoder\n",
        "    )\n",
        "    m['cluster_id'] = cid\n",
        "    m['size']       = len(idxs)\n",
        "    cluster_metrics.append(m)\n",
        "\n",
        "df_cluster = pd.DataFrame(cluster_metrics)\n",
        "display(df_cluster[['cluster_id','size','prediction_gain','reconstruction_error','sparsity','latency']])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,4))\n",
        "ax1.bar(df_cluster['cluster_id'], df_cluster['sparsity'].str.split(' ± ').str[0].astype(float))\n",
        "ax1.set_title('L1 distance by cluster')\n",
        "ax2.bar(df_cluster['cluster_id'], df_cluster['prediction_gain'].str.split(' ± ').str[0].astype(float))\n",
        "ax2.set_title('Prediction gain by cluster')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64626727",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def auto_bump_cost_weights(cluster_profiles, cluster_data, generators, feature_names,\n",
        "                           bump_factor=1.5, percentile_threshold=50):\n",
        "    \"\"\"\n",
        "    Automatically bump cost_weights for actionable features in each cluster,\n",
        "    based on per-cluster mean |delta| magnitudes.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    cluster_profiles : list of dict\n",
        "        Each dict contains 'cluster_id' and 'cost_weights' keys.\n",
        "    cluster_data : dict\n",
        "        Mapping cluster_id -> {'X': np.ndarray, ...}\n",
        "    generators : dict\n",
        "        Mapping cluster_id -> trained generator model\n",
        "    feature_names : list[str]\n",
        "        List of all feature names in X_train order.\n",
        "    bump_factor : float\n",
        "        Multiplicative factor to bump cost_weights by.\n",
        "    percentile_threshold : float\n",
        "        Delta threshold percentile (0-100). Features with mean delta above\n",
        "        this percentile get bumped.\n",
        "    \"\"\"\n",
        "    for prof in cluster_profiles:\n",
        "        cid = prof['cluster_id']\n",
        "        gen = generators[cid]\n",
        "        Xc = cluster_data[cid]['X']  # shape (n_c, n_features)\n",
        "        \n",
        "        # Generate counterfactuals for the cluster\n",
        "        cf = gen.predict(Xc)  # shape (n_c, n_features)\n",
        "        \n",
        "        # Compute mean absolute delta per feature\n",
        "        mean_abs_delta = np.mean(np.abs(cf - Xc), axis=0)\n",
        "        \n",
        "        # Identify actionable features in this profile\n",
        "        actionable = list(prof['cost_weights'].keys())\n",
        "        actionable_idxs = [feature_names.index(f) for f in actionable]\n",
        "        \n",
        "        # Determine threshold based on percentile of actionable deltas\n",
        "        actionable_deltas = mean_abs_delta[actionable_idxs]\n",
        "        thresh = np.percentile(actionable_deltas, percentile_threshold)\n",
        "        \n",
        "        # Bump cost_weights for features exceeding the threshold\n",
        "        for f, idx in zip(actionable, actionable_idxs):\n",
        "            if mean_abs_delta[idx] > thresh:\n",
        "                old_w = prof['cost_weights'][f]\n",
        "                prof['cost_weights'][f] = old_w * bump_factor\n",
        "                print(f\"Cluster {cid}: bumped '{f}' weight {old_w:.2f} → {prof['cost_weights'][f]:.2f}\")\n",
        "                \n",
        "    return cluster_profiles\n",
        "\n",
        "# Use the correct feature names from your DataFrame\n",
        "feature_names = features  # Already defined in your preprocessing function\n",
        "\n",
        "# Print feature names to verify\n",
        "print(f\"Available features: {feature_names}\")\n",
        "print(f\"Number of features: {len(feature_names)}\")\n",
        "\n",
        "# # Example usage:\n",
        "# updated_profiles = auto_bump_cost_weights(\n",
        "#     cluster_profiles=cluster_profiles,\n",
        "#     cluster_data=cluster_data,\n",
        "#     generators=generators,\n",
        "#     feature_names=features,\n",
        "#     bump_factor=1.5,\n",
        "#     percentile_threshold=50\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf6dbee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Check your semantic→index map for “Job”\n",
        "print(\"Mapped ‘Job’ index →\", feature_to_idx[\"Job\"])\n",
        "print(\"That corresponds to column:\", feature_names[ feature_to_idx[\"Job\"] ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740c0cb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ❶ Freeze classifier once before anything else\n",
        "classifier.trainable = False\n",
        "\n",
        "# ❷ Define a finer lambda grid and longer fine-tune\n",
        "to_tune = [0,4,5]\n",
        "lambda_grid = [5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
        "sweep_iters = 100\n",
        "\n",
        "def eval_lambda_for_cluster(cid, lam):\n",
        "    Xc, yc = cluster_data[cid]['X'], cluster_data[cid]['y']\n",
        "    prof   = cluster_profile_map[cid]\n",
        "    cw     = prof['cost_weights']\n",
        "    \n",
        "    # warm-start\n",
        "    gen  = load_model(f\"generators/cluster_{cid}.h5\")\n",
        "    disc = create_discriminator()\n",
        "    cgan = define_countergan(gen, disc, classifier,\n",
        "                             cost_weights=cw,\n",
        "                             sparsity_lambda=lam)\n",
        "    # fine-tune briefly\n",
        "    batches = infinite_data_stream(Xc, yc, batch_size=128)\n",
        "    train_countergan(2, 4, sweep_iters,\n",
        "                     classifier=classifier,\n",
        "                     discriminator=disc,\n",
        "                     generator=gen,\n",
        "                     countergan=cgan,\n",
        "                     batches=batches,\n",
        "                     weighted_version=True)\n",
        "    # evaluate on that cluster’s test samples\n",
        "    idxs = [i for i,tp in enumerate(test_profiles) if tp['cluster_id']==cid]\n",
        "    Xc_te = X_test[idxs]\n",
        "    cfs   = np.stack([gen.predict(x[None])[0] for x in Xc_te])\n",
        "    m     = compute_metrics(Xc_te, cfs,\n",
        "                            latencies=np.zeros(len(Xc_te)),\n",
        "                            classifier=classifier,\n",
        "                            autoencoder=autoencoder)\n",
        "    gain = float(m['prediction_gain'].split(' ± ')[0])\n",
        "    l1   = float(m['sparsity'].split(' ± ')[0])\n",
        "    return gain, l1\n",
        "\n",
        "results = {}\n",
        "for cid in to_tune:\n",
        "    rows = []\n",
        "    for lam in lambda_grid:\n",
        "        gain, l1 = eval_lambda_for_cluster(cid, lam)\n",
        "        rows.append((lam, gain, l1))\n",
        "    results[cid] = pd.DataFrame(rows, columns=['lambda','gain','l1'])\n",
        "    print(f\"\\nCluster {cid} sweep:\")\n",
        "    display(results[cid])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e2951f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ❶ Freeze classifier once before anything else\n",
        "classifier.trainable = False\n",
        "\n",
        "# ❷ Define a finer lambda grid and longer fine-tune\n",
        "to_tune = [0,4,5]\n",
        "lambda_grid = [5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
        "sweep_iters = 100\n",
        "\n",
        "def eval_lambda_for_cluster(cid, lam):\n",
        "    Xc, yc = cluster_data[cid]['X'], cluster_data[cid]['y']\n",
        "    prof   = cluster_profile_map[cid]\n",
        "    cw     = prof['cost_weights']\n",
        "    \n",
        "    # warm-start\n",
        "    gen  = load_model(f\"generators/cluster_{cid}.h5\")\n",
        "    disc = create_discriminator()\n",
        "    cgan = define_countergan(gen, disc, classifier,\n",
        "                             cost_weights=cw,\n",
        "                             sparsity_lambda=lam)\n",
        "    # fine-tune briefly\n",
        "    batches = infinite_data_stream(Xc, yc, batch_size=128)\n",
        "    train_countergan(2, 4, sweep_iters,\n",
        "                     classifier=classifier,\n",
        "                     discriminator=disc,\n",
        "                     generator=gen,\n",
        "                     countergan=cgan,\n",
        "                     batches=batches,\n",
        "                     weighted_version=True)\n",
        "    # evaluate on that cluster’s test samples\n",
        "    idxs = [i for i,tp in enumerate(test_profiles) if tp['cluster_id']==cid]\n",
        "    Xc_te = X_test[idxs]\n",
        "    cfs   = np.stack([gen.predict(x[None])[0] for x in Xc_te])\n",
        "    m     = compute_metrics(Xc_te, cfs,\n",
        "                            latencies=np.zeros(len(Xc_te)),\n",
        "                            classifier=classifier,\n",
        "                            autoencoder=autoencoder)\n",
        "    gain = float(m['prediction_gain'].split(' ± ')[0])\n",
        "    l1   = float(m['sparsity'].split(' ± ')[0])\n",
        "    return gain, l1\n",
        "\n",
        "results = {}\n",
        "for cid in to_tune:\n",
        "    rows = []\n",
        "    for lam in lambda_grid:\n",
        "        gain, l1 = eval_lambda_for_cluster(cid, lam)\n",
        "        rows.append((lam, gain, l1))\n",
        "    results[cid] = pd.DataFrame(rows, columns=['lambda','gain','l1'])\n",
        "    print(f\"\\nCluster {cid} sweep:\")\n",
        "    display(results[cid])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58745a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ❶ Freeze classifier once before anything else\n",
        "classifier.trainable = False\n",
        "\n",
        "# ❷ Define a finer lambda grid and longer fine-tune\n",
        "to_tune = [6,7,8]\n",
        "lambda_grid = [5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
        "sweep_iters = 100\n",
        "\n",
        "def eval_lambda_for_cluster(cid, lam):\n",
        "    Xc, yc = cluster_data[cid]['X'], cluster_data[cid]['y']\n",
        "    prof   = cluster_profile_map[cid]\n",
        "    cw     = prof['cost_weights']\n",
        "    \n",
        "    # warm-start\n",
        "    gen  = load_model(f\"generators/cluster_{cid}.h5\")\n",
        "    disc = create_discriminator()\n",
        "    cgan = define_countergan(gen, disc, classifier,\n",
        "                             cost_weights=cw,\n",
        "                             sparsity_lambda=lam)\n",
        "    # fine-tune briefly\n",
        "    batches = infinite_data_stream(Xc, yc, batch_size=128)\n",
        "    train_countergan(2, 4, sweep_iters,\n",
        "                     classifier=classifier,\n",
        "                     discriminator=disc,\n",
        "                     generator=gen,\n",
        "                     countergan=cgan,\n",
        "                     batches=batches,\n",
        "                     weighted_version=True)\n",
        "    # evaluate on that cluster’s test samples\n",
        "    idxs = [i for i,tp in enumerate(test_profiles) if tp['cluster_id']==cid]\n",
        "    Xc_te = X_test[idxs]\n",
        "    cfs   = np.stack([gen.predict(x[None])[0] for x in Xc_te])\n",
        "    m     = compute_metrics(Xc_te, cfs,\n",
        "                            latencies=np.zeros(len(Xc_te)),\n",
        "                            classifier=classifier,\n",
        "                            autoencoder=autoencoder)\n",
        "    gain = float(m['prediction_gain'].split(' ± ')[0])\n",
        "    l1   = float(m['sparsity'].split(' ± ')[0])\n",
        "    return gain, l1\n",
        "\n",
        "# results = {}\n",
        "# for cid in to_tune:\n",
        "#     rows = []\n",
        "#     for lam in lambda_grid:\n",
        "#         gain, l1 = eval_lambda_for_cluster(cid, lam)\n",
        "#         rows.append((lam, gain, l1))\n",
        "#     results[cid] = pd.DataFrame(rows, columns=['lambda','gain','l1'])\n",
        "#     print(f\"\\nCluster {cid} sweep:\")\n",
        "#     display(results[cid])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ffd6f16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Identify clusters with L1 > 6.0\n",
        "high_l1 = df_cluster[\n",
        "    df_cluster['sparsity'].str.split(' ± ').str[0].astype(float) > 6.0\n",
        "]['cluster_id'].tolist()\n",
        "print(\"Clusters to retune λ:\", high_l1)\n",
        "\n",
        "# 2) New λ grid (higher values)\n",
        "lambda_grid = [5e-5, 1e-4, 5e-4, 1e-3, 5e-3]\n",
        "sweep_iters = 100\n",
        "\n",
        "# 3) Re-use your eval_lambda_for_cluster\n",
        "results = {}\n",
        "for cid in high_l1:\n",
        "    rows = []\n",
        "    for lam in lambda_grid:\n",
        "        gain, l1 = eval_lambda_for_cluster(cid, lam)\n",
        "        rows.append((lam, gain, l1))\n",
        "    results[cid] = pd.DataFrame(rows, columns=['lambda','gain','l1'])\n",
        "    print(f\"\\nCluster {cid} λ-sweep:\")\n",
        "    display(results[cid])\n",
        "\n",
        "# 4) Pick for each cluster the smallest λ that brings l1 below your target\n",
        "best_lams = {}\n",
        "for cid, df in results.items():\n",
        "    cand = df[df['l1'] <= 6.0]\n",
        "    if not cand.empty:\n",
        "        best = cand.sort_values('lambda').iloc[0]['lambda']\n",
        "    else:\n",
        "        # fallback to λ that minimizes l1 (even if >6)\n",
        "        best = df.sort_values('l1').iloc[0]['lambda']\n",
        "    best_lams[cid] = best\n",
        "\n",
        "print(\"Chosen λ per cluster:\", best_lams)\n",
        "\n",
        "# 5) Inject into your JSON\n",
        "with open(\"final_user_profiles_with_metadata.json\",\"r\") as f:\n",
        "    profiles = json.load(f)\n",
        "for p in profiles:\n",
        "    cid = p['cluster_id']\n",
        "    if cid in best_lams:\n",
        "        p['sparsity_lambda'] = float(best_lams[cid])\n",
        "with open(\"final_user_profiles_with_metadata.json\",\"w\") as f:\n",
        "    json.dump(profiles, f, indent=2)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
