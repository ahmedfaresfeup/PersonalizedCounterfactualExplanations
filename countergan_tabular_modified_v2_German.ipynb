{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5mmOdOHznl"
      },
      "source": [
        "# Counterfactuals benchmark on tabular datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNbfZ4CbwI_",
        "outputId": "a152bcac-e593-4f83-e9b5-ffb9412f9cf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-01 11:03:11.113070: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-07-01 11:03:11.149115: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-07-01 11:03:11.149145: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-07-01 11:03:11.149175: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-01 11:03:11.157051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-07-01 11:03:11.769020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/ahmed/prototype\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "\n",
        "BASE_PATH = \"./counterfactuals\"\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7sVjpCaH3yD"
      },
      "source": [
        "## Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tW1SdKmbrbj",
        "outputId": "eb57f82e-de9d-4046-de2d-e154bdea3863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alibi version: 0.9.7.dev0\n",
            "Is TensorFlow running in eager execution mode? -----→ False\n",
            "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU (UUID: GPU-ed7340f2-1910-df12-4a83-29feeba52695)\n"
          ]
        }
      ],
      "source": [
        "# Install the dev version of the Alibi package if not already installed\n",
        "try:\n",
        "    from alibi import __version__ as alibi_version\n",
        "    print(f\"Alibi version: {alibi_version}\")\n",
        "except ImportError:\n",
        "    print(\"Alibi package not found, installing...\")\n",
        "    # Install the dev version of Alibi\n",
        "    !pip install git+https://github.com/SeldonIO/alibi.git > /dev/null\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "alibi_logger = logging.getLogger(\"alibi\")\n",
        "alibi_logger.setLevel(\"CRITICAL\")\n",
        "\n",
        "\n",
        "print(f\"Is TensorFlow running in eager execution mode? -----→ {tf.executing_eagerly()}\")\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6aJpXl3siWZr"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "\n",
        "\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_{date}\"\n",
        "MODELS_EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_2020-09-09\"\n",
        "if not os.path.exists(EXPERIMENT_PATH):\n",
        "    os.makedirs(EXPERIMENT_PATH)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ne3HvYI8dv"
      },
      "source": [
        "## Data import and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "mAPAZpVUiks-",
        "outputId": "f7628cfa-2bd6-4c90-e027-d90f0ecf58b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/ahmed/prototype\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# import pickle\n",
        "# import time\n",
        "# from matplotlib import offsetbox\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.layers import Dense, Add, Input, ActivityRegularization, Concatenate, Multiply\n",
        "from tensorflow.keras import optimizers, Model, regularizers, Input\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "# from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "INITIAL_CLASS = 0\n",
        "DESIRED_CLASS = 1\n",
        "N_CLASSES = 2\n",
        "n_training_iterations = 10\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "# German Credit dataset\n",
        "\n",
        "def preprocess_data_german(df, target_column=\"Outcome\"):\n",
        "    \"\"\"\n",
        "    Preprocess the German Credit dataset by encoding categorical variables and splitting the data into \n",
        "    train, test, and user simulation sets.\n",
        "    \n",
        "    Returns a dictionary with processed train, test, and user datasets.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Assign meaningful column names\n",
        "    df.columns = [\n",
        "        'Status', 'Month', 'Credit_History', 'Purpose', 'Credit_Amount',\n",
        "        'Savings', 'Employment', 'Installment_Rate', 'Personal_Status', 'Other_Debtors',\n",
        "        'Residence_Duration', 'Property', 'Age', 'Other_Installment_Plans', 'Housing',\n",
        "        'Existing_Credits', 'Job', 'Num_Liable_People', 'Telephone', 'Foreign_Worker',\n",
        "        'Outcome'\n",
        "    ]\n",
        "    \n",
        "    # Mapping categorical features to more meaningful values\n",
        "    status_mapping = { 'A11': '< 0 DM', 'A12': '0 <= ... < 200 DM', 'A13': '>= 200 DM / salary assignments for at least 1 year', 'A14': 'no checking account' }\n",
        "    credit_history_mapping = { 'A30': 'no credits taken/ all credits paid back duly', 'A31': 'all credits at this bank paid back duly', 'A32': 'existing credits paid back duly till now', 'A33': 'delay in paying off in the past', 'A34': 'critical account/other credits existing' }\n",
        "    savings_mapping = { 'A61': '< 100 DM', 'A62': '100 <= ... < 500 DM', 'A63': '500 <= ... < 1000 DM', 'A64': '>= 1000 DM', 'A65': 'unknown/no savings account' }\n",
        "    employment_mapping = { 'A71': 'unemployed', 'A72': '< 1 year', 'A73': '1 <= ... < 4 years', 'A74': '4 <= ... < 7 years', 'A75': '>= 7 years' }\n",
        "    personal_status_mapping = { 'A91': 'male: divorced/separated', 'A92': 'female: divorced/separated/married', 'A93': 'male: single', 'A94': 'male: married/widowed', 'A95': 'female: single' }\n",
        "    other_debtors_mapping = { 'A101': 'none', 'A102': 'co-applicant', 'A103': 'guarantor' }\n",
        "    property_mapping = { 'A121': 'real estate', 'A122': 'building society savings agreement/life insurance', 'A123': 'car or other, not in attribute 6', 'A124': 'unknown/no property' }\n",
        "    other_installment_plans_mapping = { 'A141': 'bank', 'A142': 'stores', 'A143': 'none' }\n",
        "    housing_mapping = { 'A151': 'rent', 'A152': 'own', 'A153': 'for free' }\n",
        "    telephone_mapping = { 'A191': 'none', 'A192': 'yes, registered under the customer\\'s name' }\n",
        "    foreign_worker_mapping = { 'A201': 'yes', 'A202': 'no' }\n",
        "\n",
        "    # Apply mappings\n",
        "    df['Status'] = df['Status'].map(status_mapping)\n",
        "    df['Credit_History'] = df['Credit_History'].map(credit_history_mapping)\n",
        "    df['Savings'] = df['Savings'].map(savings_mapping)\n",
        "    df['Employment'] = df['Employment'].map(employment_mapping)\n",
        "    df['Personal_Status'] = df['Personal_Status'].map(personal_status_mapping)\n",
        "    df['Other_Debtors'] = df['Other_Debtors'].map(other_debtors_mapping)\n",
        "    df['Property'] = df['Property'].map(property_mapping)\n",
        "    df['Other_Installment_Plans'] = df['Other_Installment_Plans'].map(other_installment_plans_mapping)\n",
        "    df['Housing'] = df['Housing'].map(housing_mapping)\n",
        "    df['Telephone'] = df['Telephone'].map(telephone_mapping)\n",
        "    df['Foreign_Worker'] = df['Foreign_Worker'].map(foreign_worker_mapping)\n",
        "\n",
        "    # Encode ordinal columns\n",
        "    ordinal_cols = ['Status', 'Credit_History', 'Savings', 'Employment']\n",
        "    le = LabelEncoder()\n",
        "    for col in ordinal_cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # One-hot encode nominal columns\n",
        "    nominal_columns = ['Purpose', 'Personal_Status', 'Other_Debtors', 'Property', \n",
        "                       'Other_Installment_Plans', 'Housing', 'Job', 'Telephone', 'Foreign_Worker']\n",
        "    df = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
        "\n",
        "    # Process target variable\n",
        "    Y = df[target_column].replace(1, 0).replace(2, 1)\n",
        "    X = df.drop(columns=[target_column])\n",
        "\n",
        "    # Get final feature set\n",
        "    # list all features\n",
        "    immutable_features = set(X.columns) - set(['Status', 'Credit_History'])\n",
        "    \n",
        "\n",
        "    mutable_features = set(X.columns) - set(immutable_features)\n",
        "    mutable_features = list(mutable_features)\n",
        "\n",
        "    features = list(mutable_features) + list(immutable_features)\n",
        "\n",
        "    return  X, Y, features, immutable_features, mutable_features\n",
        "    \n",
        "# =========================================================\n",
        "\n",
        "\n",
        "# Make sure 'german.csv' is in your project directory\n",
        "df = pd.read_csv('statlog_german_credit_data/german.data', sep=' ', skiprows=1, header=None)\n",
        "x,y, features, immutable_features, mutable_features = preprocess_data_german(df)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=2020)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "X_train = standard_scaler.fit_transform(X_train)\n",
        "X_test = standard_scaler.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vTcm3LgfKEtl"
      },
      "outputs": [],
      "source": [
        "def compute_reconstruction_error(x, autoencoder):\n",
        "    \"\"\"Compute the reconstruction error for a given autoencoder and data points.\"\"\"\n",
        "    preds = autoencoder.predict(x)\n",
        "    preds_flat = preds.reshape((preds.shape[0], -1))\n",
        "    x_flat = x.reshape((x.shape[0], -1))\n",
        "    return np.linalg.norm(x_flat - preds_flat, axis=1)\n",
        "\n",
        "def format_metric(metric):\n",
        "    \"\"\"Return a formatted version of a metric, with the confidence interval.\"\"\"\n",
        "    return f\"{metric.mean():.3f} ± {1.96*metric.std()/np.sqrt(len(metric)):.3f}\"\n",
        "\n",
        "def compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None):\n",
        "    \"\"\" Summarize the relevant metrics in a dictionary. \"\"\"\n",
        "    reconstruction_error = compute_reconstruction_error(counterfactuals, autoencoder)\n",
        "    delta = np.abs(samples-counterfactuals)\n",
        "    l1_distances = delta.reshape(delta.shape[0], -1).sum(axis=1)\n",
        "    prediction_gain = (\n",
        "        classifier.predict(counterfactuals)[:, DESIRED_CLASS] - \n",
        "        classifier.predict(samples)[:, DESIRED_CLASS]\n",
        "    )\n",
        "\n",
        "    metrics = dict()\n",
        "    metrics[\"reconstruction_error\"] = format_metric(reconstruction_error)\n",
        "    metrics[\"prediction_gain\"] = format_metric(prediction_gain)\n",
        "    metrics[\"sparsity\"] = format_metric(l1_distances)\n",
        "    metrics[\"latency\"] = format_metric(latencies)\n",
        "    batch_latency = batch_latency if batch_latency else sum(latencies)\n",
        "    metrics[\"latency_batch\"] = f\"{batch_latency:.3f}\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def save_experiment(method_name, samples, counterfactuals, latencies, \n",
        "                    batch_latency=None):\n",
        "    \"\"\"Create an experiment folder and save counterfactuals, latencies and metrics.\"\"\"\n",
        "    if not os.path.exists(f\"{EXPERIMENT_PATH}/{method_name}\"):\n",
        "        os.makedirs(f\"{EXPERIMENT_PATH}/{method_name}\")   \n",
        "\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/counterfactuals.npy\", counterfactuals)\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/latencies.npy\", latencies)\n",
        "\n",
        "    metrics = compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder)\n",
        "    json.dump(metrics, open(f\"{EXPERIMENT_PATH}/{method_name}/metrics.json\", \"w\"))\n",
        "    pprint(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw26uWLiw2c",
        "outputId": "b89cf614-ecd1-4638-9c6e-0c91a4d81f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier loaded from ./counterfactuals/diabetes_2025-07-01/classifier.keras\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Add, Input, ActivityRegularization\n",
        "from tensorflow.keras import Model, optimizers, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.random.set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "# def create_classifier(input_shape):\n",
        "#     \"\"\"Define and compile a neural network binary classifier.\"\"\" \n",
        "#     model = Sequential([\n",
        "#         Dense(20, activation='relu', input_shape=input_shape),\n",
        "#         Dense(20, activation='relu'),\n",
        "#         Dense(2, activation='softmax'),\n",
        "#     ], name=\"classifier\")\n",
        "#     optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "#     model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# classifier = create_classifier((x.shape[1],))\n",
        "# print(X_train.dtype, y_train.dtype)\n",
        "# print(X_test.dtype, y_test.dtype)\n",
        "\n",
        "# X_train = X_train.astype(np.float32)\n",
        "# X_test = X_test.astype(np.float32)\n",
        "# y_train = y_train.astype(np.float32)\n",
        "# y_test = y_test.astype(np.float32)\n",
        "# training = classifier.fit(X_train, y_train, batch_size=32, epochs=200, verbose=0,\n",
        "#                           validation_data=(X_test, y_test),)\n",
        "# print(f\"Training: loss={training.history['loss'][-1]:.4f}, \"\n",
        "#       f\"accuracy={training.history['accuracy'][-1]:.4f}\")\n",
        "# print(f\"Validation: loss={training.history['val_loss'][-1]:.4f}, \"\n",
        "#       f\"accuracy={training.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "# classifier.save(f\"{EXPERIMENT_PATH}/classifier.keras\")\n",
        "\n",
        "# Load the classifier model\n",
        "filename = f\"{EXPERIMENT_PATH}/classifier.keras\"\n",
        "classifier = load_model(filename)\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(f\"Classifier loaded from {filename}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gY6FCuwuZ75"
      },
      "source": [
        "## Estimate density with the reconstruction error of a (denoising) autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHvMh2YG3hRK",
        "outputId": "f44437fa-8c44-43c9-8b90-088cfeb07938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autoencoder loaded from ./counterfactuals/diabetes_2025-07-01/autoencoder.keras\n"
          ]
        }
      ],
      "source": [
        "# def add_noise(x, noise_factor=1e-6):\n",
        "#     x_noisy = x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape) \n",
        "#     return x_noisy\n",
        "\n",
        "    \n",
        "# def create_autoencoder(in_shape=(x.shape[1],)):\n",
        "#     input_ = Input(shape=in_shape) \n",
        "\n",
        "#     x = Dense(32, activation=\"relu\")(input_)\n",
        "#     encoded = Dense(8)(x)\n",
        "#     x = Dense(32, activation=\"relu\")(encoded)\n",
        "#     decoded = Dense(in_shape[0], activation=\"tanh\")(x)\n",
        "\n",
        "#     autoencoder = Model(input_, decoded)\n",
        "#     optimizer = optimizers.Nadam()\n",
        "#     autoencoder.compile(optimizer, 'mse')\n",
        "#     return autoencoder\n",
        "\n",
        "# autoencoder = create_autoencoder()\n",
        "# training = autoencoder.fit(\n",
        "#     add_noise(X_train), X_train, epochs=100, batch_size=32, shuffle=True, \n",
        "#     validation_data=(X_test, X_test), verbose=0\n",
        "# )\n",
        "# print(f\"Training loss: {training.history['loss'][-1]:.4f}\")\n",
        "# print(f\"Validation loss: {training.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "# n_samples = 1000\n",
        "# # Compute the reconstruction error of noise data\n",
        "# samples = np.random.randn(n_samples, X_train.shape[1])\n",
        "# reconstruction_error_noise = compute_reconstruction_error(samples, autoencoder)\n",
        "\n",
        "# # Save and print the autoencoder metrics\n",
        "# reconstruction_error = compute_reconstruction_error(X_test, autoencoder)\n",
        "# autoencoder_metrics = {\n",
        "#     \"reconstruction_error\": format_metric(reconstruction_error),\n",
        "#     \"reconstruction_error_noise\": format_metric(reconstruction_error_noise),\n",
        "# }\n",
        "# json.dump(autoencoder_metrics, open(f\"{EXPERIMENT_PATH}/autoencoder_metrics.json\", \"w\"))\n",
        "# pprint(autoencoder_metrics)\n",
        "\n",
        "# autoencoder.save(f\"{EXPERIMENT_PATH}/autoencoder.keras\")\n",
        "\n",
        "# Load the autoencoder model\n",
        "filename = f\"{EXPERIMENT_PATH}/autoencoder.keras\" \n",
        "autoencoder = load_model(filename)\n",
        "# Ensure the autoencoder is compiled with the same optimizer and loss function  \n",
        "autoencoder.compile(optimizer='nadam', loss='mse')\n",
        "\n",
        "print(f\"Autoencoder loaded from {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JcccJKG87kU"
      },
      "source": [
        "## Regularized Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dA5sU3Pf1k5",
        "outputId": "d973d943-cddd-43ef-dcd1-e27ac2ea412a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from alibi.explainers import CounterFactual\n",
        "\n",
        "# shape = (1,) + X_train.shape[1:]\n",
        "# feature_range = (X_train.min(), X_train.max())\n",
        "\n",
        "# cf = CounterFactual(classifier, shape=shape, target_proba=1.0, tol=0.5,\n",
        "#                     target_class=DESIRED_CLASS, max_iter=100, lam_init=0.001,\n",
        "#                     max_lam_steps=5, learning_rate_init=0.1,\n",
        "#                     feature_range=feature_range)\n",
        "\n",
        "# sample = X_test[1]\n",
        "\n",
        "# t_initial = time.time()\n",
        "# explanation = cf.explain(np.expand_dims(sample, axis=0))\n",
        "# print(f\"Produced explanation in {time.time() - t_initial:.2f} seconds \")\n",
        "\n",
        "# y_prob = classifier.predict(np.expand_dims(sample, axis=0))[0]\n",
        "# print(f'Original prediction: {y_prob.argmax()} with probability {y_prob.max():.3f}')\n",
        "\n",
        "# pred_class = explanation.cf['class']\n",
        "# proba = explanation.cf['proba'][0][pred_class]\n",
        "# print(f'Counterfactual prediction: {pred_class} with probability {proba:.3f}')\n",
        "\n",
        "# perturbations = (explanation.cf['X'] - sample)[0]\n",
        "# perturbations[-len(immutable_features):] = 0.\n",
        "# print(f\"Suggested perturbations: {perturbations}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "EXJ-JYqBGmxC",
        "outputId": "fec2b652-7a53-4b72-f3b9-e482e58f7888"
      },
      "outputs": [],
      "source": [
        "# samples = X_test \n",
        "\n",
        "# latencies = np.empty(len(samples))\n",
        "# counterfactuals = np.empty_like(samples)\n",
        "\n",
        "# for i, sample in enumerate(samples):\n",
        "#     if ((i % 20) == 0) or (i == (len(samples)-1)):\n",
        "#         print(f\"Iteration {i} at {datetime.now()}\")\n",
        "#     t_initial = time.time()\n",
        "#     try:\n",
        "#         explanation = cf.explain(np.expand_dims(sample, axis=0))\n",
        "#         counterfactuals[i] = explanation.cf['X']\n",
        "#     except (UnboundLocalError, TypeError):  # counterfactual search failed\n",
        "#         print(f\"{i}-th sampled failed\")\n",
        "#         counterfactuals[i] = sample\n",
        "#     latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "# print(\"Metrics before immutable features projection:\")\n",
        "# pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "#                     batch_latency=None))\n",
        "# print(\"-\"*80)\n",
        "\n",
        "# # Set immutable features to original values\n",
        "# counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "# print(\"Metrics after immutable features projection:\")\n",
        "# save_experiment(\"rgd\", samples, counterfactuals, latencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWMMH-btKLOB"
      },
      "source": [
        "## Counterfactual Search Guided by Prototypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2YxnNtLuKOHA"
      },
      "outputs": [],
      "source": [
        "# from alibi.explainers import CounterFactualProto\n",
        "\n",
        "# shape = (1,) + X_train.shape[1:]\n",
        "# feature_range = (X_train.min(), X_train.max())\n",
        "\n",
        "# cf_proto = CounterFactualProto(\n",
        "#     classifier, shape, use_kdtree=True, theta=10., feature_range=feature_range,\n",
        "#     max_iterations=200, c_steps=10\n",
        "# )\n",
        "# cf_proto.fit(X_train, trustscore_kwargs=None);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o4wfCfsKcge",
        "outputId": "0805439a-2a64-47b5-9e8c-97698abea977"
      },
      "outputs": [],
      "source": [
        "# sample = X_test[1]\n",
        "\n",
        "# t_initial = time.time()\n",
        "# explanation = cf_proto.explain(\n",
        "#     np.expand_dims(sample, axis=0), k=5, k_type='mean', target_class=[DESIRED_CLASS]\n",
        "# )\n",
        "\n",
        "# print(f\"Produced explanation in {time.time() - t_initial:.2f} seconds \")\n",
        "\n",
        "# y_prob = classifier.predict(np.expand_dims(sample, axis=0))[0]\n",
        "# print(f'Original prediction: {y_prob.argmax()} with probability {y_prob.max():.3f}')\n",
        "\n",
        "# if explanation.cf is not None:\n",
        "#     pred_class = explanation.cf['class']\n",
        "#     proba = explanation.cf['proba'][0][pred_class]\n",
        "#     print(f'Counterfactual prediction: {pred_class} with probability {proba:.3f}')\n",
        "#     perturbations = (explanation.cf['X'] - sample)[0]\n",
        "#     perturbations[-len(immutable_features):] = 0.\n",
        "#     print(f\"Suggested perturbations: {perturbations}\")\n",
        "# else:\n",
        "#     print(\"No counterfactual found for this sample.\")\n",
        "#     counterfactual = sample  # fallback to original sample\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "62zJUpDjKin9",
        "outputId": "6d825fb6-e247-47da-dfe0-3f3cd54caa82"
      },
      "outputs": [],
      "source": [
        "# verbose = False\n",
        "# samples = X_test\n",
        "\n",
        "# latencies = np.empty(len(samples))\n",
        "# counterfactuals = np.empty_like(samples)\n",
        "# for i, sample in enumerate(samples):\n",
        "#     if ((i % 20) == 0) or (i == (len(samples)-1)):\n",
        "#         print(f\"{i+1}-th iteration at {datetime.now()}\")\n",
        "#     t_initial = time.time()\n",
        "#     try:\n",
        "#         explanation = cf_proto.explain(np.expand_dims(sample, axis=0), k=20, \n",
        "#                                        k_type='mean', target_class=[DESIRED_CLASS])\n",
        "#         counterfactuals[i] = explanation.cf['X']\n",
        "#     except (UnboundLocalError, TypeError) as e:  # counterfactual search failed\n",
        "#         if verbose:\n",
        "#             print(f\"{i}-th sampled failed\")\n",
        "#         counterfactuals[i] = sample\n",
        "#     latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "# print(\"Metrics before immutable features projection:\")\n",
        "# pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "#                     batch_latency=None))\n",
        "# print(\"-\"*80)\n",
        "\n",
        "# # Set immutable features to original values\n",
        "# counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "# print(\"Metrics after immutable features projection:\")\n",
        "# save_experiment(\"csgp\", samples, counterfactuals, latencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwoisov75MsD"
      },
      "source": [
        "## GAN-based counterfactual search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gi9faGZ42qRR"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(x, generator):\n",
        "    \"\"\"Use the input generator to generate samples.\"\"\"\n",
        "    return generator.predict(x)\n",
        "\n",
        "def data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Generate batches until exhaustion of the input data.\"\"\"\n",
        "    n_train = x.shape[0]\n",
        "    if y is not None:\n",
        "        assert n_train == len(y)\n",
        "    n_complete_batches, leftover = divmod(n_train, batch_size)\n",
        "    n_batches = n_complete_batches + bool(leftover)\n",
        "\n",
        "    perm = np.random.permutation(n_train)\n",
        "    for i in range(n_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        if y is not None:\n",
        "            output = (x[batch_idx], y[batch_idx])\n",
        "        else:\n",
        "            output = x[batch_idx]\n",
        "        yield output\n",
        "\n",
        "\n",
        "def infinite_data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Infinite batch generator.\"\"\"\n",
        "    batches = data_stream(x, y, batch_size=batch_size)\n",
        "    while True:\n",
        "        try:\n",
        "            yield next(batches)\n",
        "        except StopIteration:\n",
        "            batches = data_stream(x, y, batch_size=batch_size)\n",
        "            yield next(batches)\n",
        "\n",
        "def create_generator(in_shape=(X_train.shape[1],), residuals=True):\n",
        "    \"\"\"Define and compile the residual generator of the CounteRGAN.\"\"\"\n",
        "    generator_input = Input(shape=in_shape, name='generator_input')\n",
        "    generator = Dense(64, activation='relu')(generator_input)\n",
        "    generator = Dense(32, activation='relu')(generator)\n",
        "    generator = Dense(64, activation='relu')(generator)\n",
        "    generator = Dense(in_shape[0], activation='tanh')(generator)\n",
        "    generator_output = ActivityRegularization(l1=0., l2=1e-6)(generator)\n",
        "    \n",
        "    if residuals:\n",
        "        generator_output = Add(name=\"output\")([generator_input, generator_output])\n",
        "\n",
        "    return Model(inputs=generator_input, outputs=generator_output)\n",
        "\n",
        "\n",
        "def create_discriminator(in_shape=(X_train.shape[1],)):\n",
        "    \"\"\" Define a neural network binary classifier to classify real and generated \n",
        "    examples.\"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=in_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ], name=\"discriminator\")\n",
        "    optimizer = optimizers.legacy.Adam(learning_rate=0.0005, beta_1=0.5, decay=1e-8)\n",
        "    model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def define_countergan(generator, discriminator, classifier, \n",
        "                      input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator, discriminator, and fixed classifier into the CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(\n",
        "        inputs=countergan_input, \n",
        "        outputs=[discriminator(x_generated), classifier(x_generated)]\n",
        "    )\n",
        "        \n",
        "    optimizer = optimizers.legacy.RMSprop(learning_rate=2e-4, decay=1e-8)\n",
        "    countergan.compile(optimizer, [\"binary_crossentropy\", \"categorical_crossentropy\"])\n",
        "    return countergan\n",
        "\n",
        "\n",
        "def define_weighted_countergan(generator, discriminator, \n",
        "                               input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator and a discriminator for the weighted version of the \n",
        "    CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(inputs=countergan_input, outputs=discriminator(x_generated))\n",
        "    optimizer = optimizers.legacy.RMSprop(learning_rate=5e-4, decay=1e-8)\n",
        "    countergan.compile(optimizer, \"binary_crossentropy\")  \n",
        "    return countergan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ufnw6FKdPpa2"
      },
      "outputs": [],
      "source": [
        "def train_countergan(n_discriminator_steps, n_generator_steps, n_training_iterations,\n",
        "                     classifier, discriminator, generator, batches, \n",
        "                     weighted_version=False):\n",
        "    \"\"\" Main function: train the CounteRGAN\"\"\"\n",
        "    def check_divergence(x_generated):\n",
        "        return np.all(np.isnan(x_generated))\n",
        "\n",
        "    def print_training_information(generator, classifier, X_test, iteration):\n",
        "        X_gen = generator.predict(X_test)\n",
        "        clf_pred_test = classifier.predict(X_test)\n",
        "        clf_pred = classifier.predict(X_gen)\n",
        "\n",
        "        delta_clf_pred = (clf_pred - clf_pred_test)[:, DESIRED_CLASS]\n",
        "        y_target = to_categorical([DESIRED_CLASS] * len(clf_pred), \n",
        "                                  num_classes=N_CLASSES)\n",
        "        print('='*88)\n",
        "        print(f\"Training iteration {iteration} at {datetime.now()}\")\n",
        "        \n",
        "        \n",
        "        reconstruction_error = np.mean(compute_reconstruction_error(X_gen, autoencoder))\n",
        "        print(f\"Autoencoder reconstruction error (infinity to 0): {reconstruction_error:.3f}\")\n",
        "        print(f\"Counterfactual prediction gain (0 to 1): {delta_clf_pred.mean():.3f}\")\n",
        "        print(f\"Sparsity (L1, infinity to 0): {np.mean(np.abs(X_gen-X_test)):.3f}\")\n",
        "\n",
        "    if weighted_version:\n",
        "        countergan = define_weighted_countergan(generator, discriminator)\n",
        "    else:\n",
        "        countergan = define_countergan(generator, discriminator, classifier)\n",
        "\n",
        "    for iteration in range(n_training_iterations):\n",
        "        if iteration > 0:\n",
        "            x_generated = generator.predict(x_fake_input)\n",
        "            if check_divergence(x_generated):\n",
        "                print(\"Training diverged with the following loss functions:\")\n",
        "                print(discrim_loss_1, discrim_accuracy, gan_loss, \n",
        "                    discrim_loss, discrim_loss_2, clf_loss)\n",
        "                break\n",
        "\n",
        "        # Periodically print and plot training information \n",
        "        if (iteration % 1000 == 0) or (iteration == n_training_iterations - 1):\n",
        "            print_training_information(generator, classifier, X_test, iteration)\n",
        "\n",
        "        # Train the discriminator\n",
        "        discriminator.trainable = True\n",
        "        for _ in range(n_discriminator_steps):\n",
        "            x_fake_input, _ = next(batches)\n",
        "            x_fake = generate_fake_samples(x_fake_input, generator)\n",
        "            x_real = x_fake_input\n",
        "\n",
        "            x_batch = np.concatenate([x_real, x_fake])\n",
        "            y_batch = np.concatenate([np.ones(len(x_real)), np.zeros(len(x_fake))])\n",
        "            \n",
        "            # Shuffle real and fake examples\n",
        "            p = np.random.permutation(len(y_batch))\n",
        "            x_batch, y_batch = x_batch[p], y_batch[p]\n",
        "\n",
        "            if weighted_version:\n",
        "                classifier_scores = classifier.predict(x_batch)[:, DESIRED_CLASS]\n",
        "                \n",
        "                # The following update to the classifier scores is needed to have the \n",
        "                # same order of magnitude between real and generated samples losses\n",
        "                real_samples = np.where(y_batch == 1.)\n",
        "                average_score_real_samples = np.mean(classifier_scores[real_samples])\n",
        "                classifier_scores[real_samples] /= average_score_real_samples\n",
        "                \n",
        "                fake_samples = np.where(y_batch == 0.)\n",
        "                classifier_scores[fake_samples] = 1.\n",
        "\n",
        "                discriminator.train_on_batch(\n",
        "                    x_batch, y_batch, sample_weight=classifier_scores\n",
        "                )\n",
        "            else:\n",
        "                discriminator.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "        # Train the generator \n",
        "        discriminator.trainable = False\n",
        "        for _ in range(n_generator_steps):\n",
        "            x_fake_input, _ = next(batches)\n",
        "            y_fake = np.ones(len(x_fake_input))\n",
        "            if weighted_version:\n",
        "                countergan.train_on_batch(x_fake_input, y_fake)\n",
        "            else:\n",
        "                y_target = to_categorical([DESIRED_CLASS] * len(x_fake_input), \n",
        "                                          num_classes=N_CLASSES)\n",
        "                countergan.train_on_batch(x_fake_input, [y_fake, y_target])\n",
        "    return countergan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpaZptCP2k_U"
      },
      "source": [
        "## Counterfactual search with a regular GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "L4PL01njRnV9",
        "outputId": "dbaea06d-6f95-4bf7-c02e-15daaeed5164"
      },
      "outputs": [],
      "source": [
        "# discriminator = create_discriminator()\n",
        "# generator = create_generator(residuals=False)\n",
        "# batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "\n",
        "# method_name = \"regular_gan\"\n",
        "# countergan = train_countergan(2, 4, 2000, classifier, discriminator, generator, batches)\n",
        "\n",
        "# t_initial = time.time()\n",
        "# counterfactuals = generator.predict(X_test)\n",
        "# batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "# latencies = np.zeros(len(X_test))\n",
        "# for i, x in enumerate(X_test):\n",
        "#     t_initial = time.time()\n",
        "#     _ = generator.predict(np.expand_dims(x, axis=0))\n",
        "#     latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "# print(\"-\"*80)\n",
        "# print(\"Metrics before immutable features projection:\")\n",
        "# pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "#                     batch_latency=None))\n",
        "# print(\"-\"*80)\n",
        "\n",
        "# # Set immutable features to original values\n",
        "# # Fix immutable features\n",
        "# counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "# # Re-evaluate\n",
        "# print(\"Metrics after immutable features projection:\")\n",
        "# # pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "# #                        batch_latency=batch_latency))\n",
        "# save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "# generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "# discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "# countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IvGXenSWL-"
      },
      "source": [
        "## CounteRGAN: first formulation for differentiable classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "mNQmxFS5SXIH",
        "outputId": "8ab0b191-abd2-44f3-ecd6-f980df21bd1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-01 11:03:14.794308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:14.849013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:14.849074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:14.855786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:14.855858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:14.855890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.058635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.058725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.058734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2025-07-01 11:03:15.058774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.058797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2051 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-07-01 11:03:15.210514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.210605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.210644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.210853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.210885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2025-07-01 11:03:15.210948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-01 11:03:15.210981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2051 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2025-07-01 11:03:15.224481: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "2025-07-01 11:03:15.459762: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_3_1/bias/Assign' id:447 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_3_1/bias, dense_3_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-01 11:03:16.478909: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2/Softmax' id:74 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_2/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-01 11:03:16.516795: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_6/Tanh' id:245 op device:{requested: '', assigned: ''} def:{{{node dense_6/Tanh}} = Tanh[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_6/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-01 11:03:16.630059: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_2/mul' id:404 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-01 11:03:16.652474: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/dense_1_1/bias/m/Assign' id:908 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_1_1/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_1_1/bias/m, training/Adam/dense_1_1/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Training iteration 0 at 2025-07-01 11:03:16.515406\n",
            "Autoencoder reconstruction error (infinity to 0): 6.419\n",
            "Counterfactual prediction gain (0 to 1): 0.016\n",
            "Sparsity (L1, infinity to 0): 0.256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-01 11:03:17.688367: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_3/AddN' id:710 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=3, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1, model/activity_regularization/ActivityRegularizer/truediv)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-01 11:03:17.716441: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/RMSprop/dense_3_1/kernel/rms/Assign' id:1248 op device:{requested: '', assigned: ''} def:{{{node training_2/RMSprop/dense_3_1/kernel/rms/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/RMSprop/dense_3_1/kernel/rms, training_2/RMSprop/dense_3_1/kernel/rms/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Training iteration 1000 at 2025-07-01 11:04:36.664400\n",
            "Autoencoder reconstruction error (infinity to 0): 6.096\n",
            "Counterfactual prediction gain (0 to 1): 0.049\n",
            "Sparsity (L1, infinity to 0): 0.295\n",
            "========================================================================================\n",
            "Training iteration 1999 at 2025-07-01 11:05:53.414342\n",
            "Autoencoder reconstruction error (infinity to 0): 6.156\n",
            "Counterfactual prediction gain (0 to 1): 0.039\n",
            "Sparsity (L1, infinity to 0): 0.210\n",
            "--------------------------------------------------------------------------------\n",
            "Metrics before immutable features projection:\n",
            "{'latency': '1.171 ± 0.029',\n",
            " 'latency_batch': '234.203',\n",
            " 'prediction_gain': '0.036 ± 0.010',\n",
            " 'reconstruction_error': '6.155 ± 0.242',\n",
            " 'sparsity': '7.753 ± 0.310'}\n",
            "--------------------------------------------------------------------------------\n",
            "Metrics after immutable features projection:\n",
            "{'latency': '1.171 ± 0.029',\n",
            " 'latency_batch': '234.203',\n",
            " 'prediction_gain': '0.009 ± 0.003',\n",
            " 'reconstruction_error': '6.137 ± 0.234',\n",
            " 'sparsity': '0.683 ± 0.049'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "discriminator = create_discriminator()\n",
        "generator = create_generator(residuals=True)\n",
        "batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "samples = X_test \n",
        "\n",
        "method_name = \"countergan\"\n",
        "countergan = train_countergan(2, 4, 2000, classifier, discriminator, generator, batches)\n",
        "\n",
        "t_initial = time.time()\n",
        "counterfactuals = generator.predict(X_test)\n",
        "batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "latencies = np.zeros(len(X_test))\n",
        "for i, x in enumerate(X_test):\n",
        "    t_initial = time.time()\n",
        "    _ = generator.predict(np.expand_dims(x, axis=0))\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pf5VJKqTBfd"
      },
      "source": [
        "## CounteRGAN: second formulation for any classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "LKjswCOCTC-4",
        "outputId": "036b03e9-76ac-4f42-ac37-e85f0bbf28a6"
      },
      "outputs": [],
      "source": [
        "# discriminator = create_discriminator()\n",
        "# generator = create_generator(residuals=True)\n",
        "# batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "\n",
        "# method_name = \"countergan-wt\"\n",
        "# countergan = train_countergan(2, 3, 2000, classifier, discriminator, generator, \n",
        "#                               batches, weighted_version=True)\n",
        "\n",
        "# t_initial = time.time()\n",
        "# counterfactuals = generator.predict(X_test)\n",
        "# batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "# latencies = np.zeros(len(X_test))\n",
        "# for i, x in enumerate(X_test):\n",
        "#     t_initial = time.time()\n",
        "#     _ = countergan.predict(np.expand_dims(x, axis=0))\n",
        "#     latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "# print(\"-\"*80)\n",
        "# print(\"Metrics before immutable features projection:\")\n",
        "# pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "#                     batch_latency=None))\n",
        "# print(\"-\"*80)\n",
        "\n",
        "# # Set immutable features to original values\n",
        "# counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "# print(\"Metrics after immutable features projection:\")\n",
        "# save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "# generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "# discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "# countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T-D0Jj0LVJM"
      },
      "source": [
        "## Generate the benchmark table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "5w-2mCNALc8O",
        "outputId": "5adf7fa7-ef41-43d5-df19-c6f1eabfabf2"
      },
      "outputs": [],
      "source": [
        "# METHODS = [\"rgd\", \"csgp\", \"regular_gan\", \"countergan\", \"countergan-wt\"]\n",
        "# METRIC_NAMES = [\n",
        "#     \"prediction_gain\", \"reconstruction_error\", \"sparsity\", \"latency\", \"latency_batch\"\n",
        "# ]\n",
        "\n",
        "# metrics = dict()\n",
        "# for method in METHODS:\n",
        "#     method_metrics = json.load(open(f\"{EXPERIMENT_PATH}/{method}/metrics.json\", \"r\"))\n",
        "#     method_metrics = {k: v for k, v in method_metrics.items() if k in METRIC_NAMES}\n",
        "#     metrics[method] = method_metrics\n",
        "\n",
        "# metrics = pd.DataFrame(metrics)\n",
        "# metrics.columns =  [\"RGD\",  \"CSGP\", \"GAN\", \"CounterGAN\", \"CounterRGAN-wt\"] \n",
        "\n",
        "# metrics.index = [\n",
        "#     \"↓ Realism\",\n",
        "#     \"↑ Prediction gain\",\n",
        "#     \"↓ Sparsity\",\n",
        "#     \"↓ Latency (ms)\",\n",
        "#     \"↓ Batch latency (ms)\",\n",
        "# ]\n",
        "\n",
        "# metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKAfN55k9zjd"
      },
      "source": [
        "## Individual examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "s69m-GMnXxNf",
        "outputId": "facf23e6-0098-4842-b783-d5cc91218964"
      },
      "outputs": [],
      "source": [
        "# negative_idx = np.where(classifier.predict(X_test)[:, 1] < 0.5)[0]\n",
        "# x_negative = X_test[negative_idx]\n",
        "# original_features = standard_scaler.inverse_transform(x_negative)\n",
        "# negative_df = pd.DataFrame(original_features, columns=features)\n",
        "# negative_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Ud0Dwe0GZXFT",
        "outputId": "5c247d8b-e9a7-42e2-95d0-35492781d306"
      },
      "outputs": [],
      "source": [
        "# counterfactuals = standard_scaler.inverse_transform(\n",
        "#     generator.predict(X_test[negative_idx])\n",
        "# )\n",
        "# residuals = (counterfactuals - \n",
        "#              standard_scaler.inverse_transform(X_test[negative_idx]))\n",
        "# residuals_df = pd.DataFrame(residuals, columns=features)\n",
        "# residuals_df[list(immutable_features)] = 0.\n",
        "# residuals_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "qW9zy0bTYM5h",
        "outputId": "e74d92aa-3aa2-4d08-80d6-7a836c7b86c5"
      },
      "outputs": [],
      "source": [
        "# sample_idx = 20\n",
        "# sample = np.expand_dims(X_test[sample_idx], axis=0)\n",
        "\n",
        "# def compute_residuals(sample, counterfactual):\n",
        "#     counterfactual = standard_scaler.inverse_transform(counterfactual)\n",
        "#     residuals = (counterfactual - standard_scaler.inverse_transform(sample))[0]\n",
        "#     residuals[-len(immutable_features):] = 0\n",
        "#     return residuals\n",
        "\n",
        "# method_outputs = dict()\n",
        "\n",
        "# d = negative_df.iloc[sample_idx].to_dict()\n",
        "# d[\"Classifier Prediction\"] = classifier.predict(sample)[0][1]\n",
        "# method_outputs[\"Initial values\"] = d\n",
        "\n",
        "\n",
        "# explanation = cf.explain(sample)\n",
        "# counterfactual = explanation.cf['X']\n",
        "# scaled_counterfactual = compute_residuals(sample, counterfactual)\n",
        "# d = {k: v for k, v in zip(features, list(scaled_counterfactual))}\n",
        "# d[\"Classifier Prediction\"] = classifier.predict(counterfactual)[0][1]\n",
        "# method_outputs[\"RGD\"] = d\n",
        "\n",
        "# explanation = cf_proto.explain(sample, k=5, k_type='mean', target_class=[DESIRED_CLASS])\n",
        "# counterfactual = explanation.cf['X']\n",
        "# scaled_counterfactual = compute_residuals(sample, counterfactual)\n",
        "# d = {k: v for k, v in zip(features, list(scaled_counterfactual))}\n",
        "# d[\"Classifier Prediction\"] = classifier.predict(counterfactual)[0][1]\n",
        "# method_outputs[\"CSGP\"] = d\n",
        "\n",
        "# for method in [\"regular_gan\", \"countergan\", \"countergan-wt\"]:\n",
        "#     generator = load_model(f\"{EXPERIMENT_PATH}/{method}/generator.h5\")\n",
        "#     counterfactual = generator.predict(sample)\n",
        "#     scaled_counterfactual = compute_residuals(sample, counterfactual)\n",
        "#     d = {k: v for k, v in zip(features, list(scaled_counterfactual))}\n",
        "#     d[\"Classifier Prediction\"] = classifier.predict(counterfactual)[0][1]\n",
        "#     method_outputs[method] = d\n",
        "\n",
        "# df = pd.DataFrame(method_outputs)\n",
        "# df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "kdd_counterfactuals_diabetes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
