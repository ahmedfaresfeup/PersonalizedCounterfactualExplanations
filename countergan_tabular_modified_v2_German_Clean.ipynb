{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5mmOdOHznl"
      },
      "source": [
        "# Counterfactuals benchmark on tabular datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNbfZ4CbwI_",
        "outputId": "a152bcac-e593-4f83-e9b5-ffb9412f9cf9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdisable_eager_execution()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "# tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "\n",
        "BASE_PATH = \"./counterfactuals\"\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7sVjpCaH3yD"
      },
      "source": [
        "## Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tW1SdKmbrbj",
        "outputId": "eb57f82e-de9d-4046-de2d-e154bdea3863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alibi version: 0.9.7.dev0\n",
            "Is TensorFlow running in eager execution mode? -----→ False\n",
            "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU (UUID: GPU-ed7340f2-1910-df12-4a83-29feeba52695)\n"
          ]
        }
      ],
      "source": [
        "# Install the dev version of the Alibi package if not already installed\n",
        "try:\n",
        "    from alibi import __version__ as alibi_version\n",
        "    print(f\"Alibi version: {alibi_version}\")\n",
        "except ImportError:\n",
        "    print(\"Alibi package not found, installing...\")\n",
        "    # Install the dev version of Alibi\n",
        "    !pip install git+https://github.com/SeldonIO/alibi.git > /dev/null\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "alibi_logger = logging.getLogger(\"alibi\")\n",
        "alibi_logger.setLevel(\"CRITICAL\")\n",
        "\n",
        "\n",
        "print(f\"Is TensorFlow running in eager execution mode? -----→ {tf.executing_eagerly()}\")\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6aJpXl3siWZr"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "\n",
        "\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_{date}\"\n",
        "MODELS_EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_2020-09-09\"\n",
        "if not os.path.exists(EXPERIMENT_PATH):\n",
        "    os.makedirs(EXPERIMENT_PATH)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ne3HvYI8dv"
      },
      "source": [
        "## Data import and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "mAPAZpVUiks-",
        "outputId": "f7628cfa-2bd6-4c90-e027-d90f0ecf58b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/ahmed/prototype\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# import pickle\n",
        "# import time\n",
        "# from matplotlib import offsetbox\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.layers import Dense, Add, Input, ActivityRegularization, Concatenate, Multiply\n",
        "from tensorflow.keras import optimizers, Model, regularizers, Input\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "# from tensorflow.keras.models import load_model\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "INITIAL_CLASS = 0\n",
        "DESIRED_CLASS = 1\n",
        "N_CLASSES = 2\n",
        "n_training_iterations = 10\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "# German Credit dataset\n",
        "\n",
        "def preprocess_data_german(df, target_column=\"Outcome\"):\n",
        "    \"\"\"\n",
        "    Preprocess the German Credit dataset by encoding categorical variables and splitting the data into \n",
        "    train, test, and user simulation sets.\n",
        "    \n",
        "    Returns a dictionary with processed train, test, and user datasets.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Assign meaningful column names\n",
        "    df.columns = [\n",
        "        'Status', 'Month', 'Credit_History', 'Purpose', 'Credit_Amount',\n",
        "        'Savings', 'Employment', 'Installment_Rate', 'Personal_Status', 'Other_Debtors',\n",
        "        'Residence_Duration', 'Property', 'Age', 'Other_Installment_Plans', 'Housing',\n",
        "        'Existing_Credits', 'Job', 'Num_Liable_People', 'Telephone', 'Foreign_Worker',\n",
        "        'Outcome'\n",
        "    ]\n",
        "    \n",
        "    # Mapping categorical features to more meaningful values\n",
        "    status_mapping = { 'A11': '< 0 DM', 'A12': '0 <= ... < 200 DM', 'A13': '>= 200 DM / salary assignments for at least 1 year', 'A14': 'no checking account' }\n",
        "    credit_history_mapping = { 'A30': 'no credits taken/ all credits paid back duly', 'A31': 'all credits at this bank paid back duly', 'A32': 'existing credits paid back duly till now', 'A33': 'delay in paying off in the past', 'A34': 'critical account/other credits existing' }\n",
        "    savings_mapping = { 'A61': '< 100 DM', 'A62': '100 <= ... < 500 DM', 'A63': '500 <= ... < 1000 DM', 'A64': '>= 1000 DM', 'A65': 'unknown/no savings account' }\n",
        "    employment_mapping = { 'A71': 'unemployed', 'A72': '< 1 year', 'A73': '1 <= ... < 4 years', 'A74': '4 <= ... < 7 years', 'A75': '>= 7 years' }\n",
        "    personal_status_mapping = { 'A91': 'male: divorced/separated', 'A92': 'female: divorced/separated/married', 'A93': 'male: single', 'A94': 'male: married/widowed', 'A95': 'female: single' }\n",
        "    other_debtors_mapping = { 'A101': 'none', 'A102': 'co-applicant', 'A103': 'guarantor' }\n",
        "    property_mapping = { 'A121': 'real estate', 'A122': 'building society savings agreement/life insurance', 'A123': 'car or other, not in attribute 6', 'A124': 'unknown/no property' }\n",
        "    other_installment_plans_mapping = { 'A141': 'bank', 'A142': 'stores', 'A143': 'none' }\n",
        "    housing_mapping = { 'A151': 'rent', 'A152': 'own', 'A153': 'for free' }\n",
        "    telephone_mapping = { 'A191': 'none', 'A192': 'yes, registered under the customer\\'s name' }\n",
        "    foreign_worker_mapping = { 'A201': 'yes', 'A202': 'no' }\n",
        "\n",
        "    # Apply mappings\n",
        "    df['Status'] = df['Status'].map(status_mapping)\n",
        "    df['Credit_History'] = df['Credit_History'].map(credit_history_mapping)\n",
        "    df['Savings'] = df['Savings'].map(savings_mapping)\n",
        "    df['Employment'] = df['Employment'].map(employment_mapping)\n",
        "    df['Personal_Status'] = df['Personal_Status'].map(personal_status_mapping)\n",
        "    df['Other_Debtors'] = df['Other_Debtors'].map(other_debtors_mapping)\n",
        "    df['Property'] = df['Property'].map(property_mapping)\n",
        "    df['Other_Installment_Plans'] = df['Other_Installment_Plans'].map(other_installment_plans_mapping)\n",
        "    df['Housing'] = df['Housing'].map(housing_mapping)\n",
        "    df['Telephone'] = df['Telephone'].map(telephone_mapping)\n",
        "    df['Foreign_Worker'] = df['Foreign_Worker'].map(foreign_worker_mapping)\n",
        "\n",
        "    # Encode ordinal columns\n",
        "    ordinal_cols = ['Status', 'Credit_History', 'Savings', 'Employment']\n",
        "    le = LabelEncoder()\n",
        "    for col in ordinal_cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # One-hot encode nominal columns\n",
        "    nominal_columns = ['Purpose', 'Personal_Status', 'Other_Debtors', 'Property', \n",
        "                       'Other_Installment_Plans', 'Housing', 'Job', 'Telephone', 'Foreign_Worker']\n",
        "    df = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
        "\n",
        "    # Process target variable\n",
        "    Y = df[target_column].replace(1, 0).replace(2, 1)\n",
        "    X = df.drop(columns=[target_column])\n",
        "\n",
        "    # Get final feature set\n",
        "    # list all features\n",
        "    immutable_features = set(X.columns) - set(['Status', 'Credit_History'])\n",
        "    \n",
        "\n",
        "    mutable_features = set(X.columns) - set(immutable_features)\n",
        "    mutable_features = list(mutable_features)\n",
        "\n",
        "    features = list(mutable_features) + list(immutable_features)\n",
        "\n",
        "    return  X, Y, features, immutable_features, mutable_features\n",
        "    \n",
        "# =========================================================\n",
        "\n",
        "\n",
        "# Make sure 'german.csv' is in your project directory\n",
        "df = pd.read_csv('statlog_german_credit_data/german.data', sep=' ', skiprows=1, header=None)\n",
        "x,y, features, immutable_features, mutable_features = preprocess_data_german(df)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=2020)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "X_train = standard_scaler.fit_transform(X_train)\n",
        "X_test = standard_scaler.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vTcm3LgfKEtl"
      },
      "outputs": [],
      "source": [
        "def compute_reconstruction_error(x, autoencoder):\n",
        "    \"\"\"Compute the reconstruction error for a given autoencoder and data points.\"\"\"\n",
        "    preds = autoencoder.predict(x)\n",
        "    preds_flat = preds.reshape((preds.shape[0], -1))\n",
        "    x_flat = x.reshape((x.shape[0], -1))\n",
        "    return np.linalg.norm(x_flat - preds_flat, axis=1)\n",
        "\n",
        "def format_metric(metric):\n",
        "    \"\"\"Return a formatted version of a metric, with the confidence interval.\"\"\"\n",
        "    return f\"{metric.mean():.3f} ± {1.96*metric.std()/np.sqrt(len(metric)):.3f}\"\n",
        "\n",
        "def compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None):\n",
        "    \"\"\" Summarize the relevant metrics in a dictionary. \"\"\"\n",
        "    reconstruction_error = compute_reconstruction_error(counterfactuals, autoencoder)\n",
        "    delta = np.abs(samples-counterfactuals)\n",
        "    l1_distances = delta.reshape(delta.shape[0], -1).sum(axis=1)\n",
        "    prediction_gain = (\n",
        "        classifier.predict(counterfactuals)[:, DESIRED_CLASS] - \n",
        "        classifier.predict(samples)[:, DESIRED_CLASS]\n",
        "    )\n",
        "\n",
        "    metrics = dict()\n",
        "    metrics[\"reconstruction_error\"] = format_metric(reconstruction_error)\n",
        "    metrics[\"prediction_gain\"] = format_metric(prediction_gain)\n",
        "    metrics[\"sparsity\"] = format_metric(l1_distances)\n",
        "    metrics[\"latency\"] = format_metric(latencies)\n",
        "    batch_latency = batch_latency if batch_latency else sum(latencies)\n",
        "    metrics[\"latency_batch\"] = f\"{batch_latency:.3f}\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def save_experiment(method_name, samples, counterfactuals, latencies, \n",
        "                    batch_latency=None):\n",
        "    \"\"\"Create an experiment folder and save counterfactuals, latencies and metrics.\"\"\"\n",
        "    if not os.path.exists(f\"{EXPERIMENT_PATH}/{method_name}\"):\n",
        "        os.makedirs(f\"{EXPERIMENT_PATH}/{method_name}\")   \n",
        "\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/counterfactuals.npy\", counterfactuals)\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/latencies.npy\", latencies)\n",
        "\n",
        "    metrics = compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder)\n",
        "    json.dump(metrics, open(f\"{EXPERIMENT_PATH}/{method_name}/metrics.json\", \"w\"))\n",
        "    pprint(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw26uWLiw2c",
        "outputId": "b89cf614-ecd1-4638-9c6e-0c91a4d81f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
            "Classifier loaded from ./counterfactuals/diabetes_2025-07-12/classifier.keras\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Add, Input, ActivityRegularization\n",
        "from tensorflow.keras import Model, optimizers, regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.random.set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "\n",
        "# Load the classifier model\n",
        "filename = f\"{EXPERIMENT_PATH}/classifier.keras\"\n",
        "classifier = load_model(filename)\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(f\"Classifier loaded from {filename}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gY6FCuwuZ75"
      },
      "source": [
        "## Estimate density with the reconstruction error of a (denoising) autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHvMh2YG3hRK",
        "outputId": "f44437fa-8c44-43c9-8b90-088cfeb07938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
            "Autoencoder loaded from ./counterfactuals/diabetes_2025-07-12/autoencoder.keras\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the autoencoder model\n",
        "filename = f\"{EXPERIMENT_PATH}/autoencoder.keras\" \n",
        "autoencoder = load_model(filename)\n",
        "# Ensure the autoencoder is compiled with the same optimizer and loss function  \n",
        "autoencoder.compile(optimizer='nadam', loss='mse')\n",
        "\n",
        "print(f\"Autoencoder loaded from {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwoisov75MsD"
      },
      "source": [
        "## GAN-based counterfactual search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gi9faGZ42qRR"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(x, generator):\n",
        "    \"\"\"Use the input generator to generate samples.\"\"\"\n",
        "    return generator.predict(x)\n",
        "\n",
        "def data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Generate batches until exhaustion of the input data.\"\"\"\n",
        "    n_train = x.shape[0]\n",
        "    if y is not None:\n",
        "        assert n_train == len(y)\n",
        "    n_complete_batches, leftover = divmod(n_train, batch_size)\n",
        "    n_batches = n_complete_batches + bool(leftover)\n",
        "\n",
        "    perm = np.random.permutation(n_train)\n",
        "    for i in range(n_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        if y is not None:\n",
        "            output = (x[batch_idx], y[batch_idx])\n",
        "        else:\n",
        "            output = x[batch_idx]\n",
        "        yield output\n",
        "\n",
        "\n",
        "def infinite_data_stream(x, y=None, batch_size=500):\n",
        "    \"\"\"Infinite batch generator.\"\"\"\n",
        "    batches = data_stream(x, y, batch_size=batch_size)\n",
        "    while True:\n",
        "        try:\n",
        "            yield next(batches)\n",
        "        except StopIteration:\n",
        "            batches = data_stream(x, y, batch_size=batch_size)\n",
        "            yield next(batches)\n",
        "\n",
        "def create_generator(in_shape=(X_train.shape[1],), residuals=True):\n",
        "    \"\"\"Define and compile the residual generator of the CounteRGAN.\"\"\"\n",
        "    generator_input = Input(shape=in_shape, name='generator_input')\n",
        "    generator = Dense(64, activation='relu')(generator_input)\n",
        "    generator = Dense(32, activation='relu')(generator)\n",
        "    generator = Dense(64, activation='relu')(generator)\n",
        "    generator = Dense(in_shape[0], activation='tanh')(generator)\n",
        "    generator_output = ActivityRegularization(l1=0., l2=1e-6)(generator)\n",
        "    \n",
        "    if residuals:\n",
        "        generator_output = Add(name=\"output\")([generator_input, generator_output])\n",
        "\n",
        "    return Model(inputs=generator_input, outputs=generator_output)\n",
        "\n",
        "\n",
        "def create_discriminator(in_shape=(X_train.shape[1],)):\n",
        "    \"\"\" Define a neural network binary classifier to classify real and generated \n",
        "    examples.\"\"\"\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=in_shape),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ], name=\"discriminator\")\n",
        "    optimizer = optimizers.legacy.Adam(learning_rate=0.0005, beta_1=0.5, decay=1e-8)\n",
        "    model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def define_countergan(generator, discriminator, classifier, \n",
        "                      input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator, discriminator, and fixed classifier into the CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(\n",
        "        inputs=countergan_input, \n",
        "        outputs=[discriminator(x_generated), classifier(x_generated)]\n",
        "    )\n",
        "        \n",
        "    optimizer = optimizers.legacy.RMSprop(learning_rate=2e-4, decay=1e-8)\n",
        "    countergan.compile(optimizer, [\"binary_crossentropy\", \"categorical_crossentropy\"])\n",
        "    return countergan\n",
        "\n",
        "\n",
        "def define_weighted_countergan(generator, discriminator, \n",
        "                               input_shape=(X_train.shape[1],)):\n",
        "    \"\"\"Combine a generator and a discriminator for the weighted version of the \n",
        "    CounteRGAN.\"\"\"\n",
        "    discriminator.trainable = False\n",
        "    classifier.trainable = False\n",
        "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
        "  \n",
        "    x_generated = generator(countergan_input)\n",
        "\n",
        "    countergan = Model(inputs=countergan_input, outputs=discriminator(x_generated))\n",
        "    optimizer = optimizers.legacy.RMSprop(learning_rate=5e-4, decay=1e-8)\n",
        "    countergan.compile(optimizer, \"binary_crossentropy\")  \n",
        "    return countergan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ufnw6FKdPpa2"
      },
      "outputs": [],
      "source": [
        "def train_countergan(n_discriminator_steps, n_generator_steps, n_training_iterations,\n",
        "                     classifier, discriminator, generator, batches, \n",
        "                     weighted_version=False):\n",
        "    \"\"\" Main function: train the CounteRGAN\"\"\"\n",
        "    def check_divergence(x_generated):\n",
        "        return np.all(np.isnan(x_generated))\n",
        "\n",
        "    def print_training_information(generator, classifier, X_test, iteration):\n",
        "        X_gen = generator.predict(X_test)\n",
        "        clf_pred_test = classifier.predict(X_test)\n",
        "        clf_pred = classifier.predict(X_gen)\n",
        "\n",
        "        delta_clf_pred = (clf_pred - clf_pred_test)[:, DESIRED_CLASS]\n",
        "        y_target = to_categorical([DESIRED_CLASS] * len(clf_pred), \n",
        "                                  num_classes=N_CLASSES)\n",
        "        print('='*88)\n",
        "        print(f\"Training iteration {iteration} at {datetime.now()}\")\n",
        "        \n",
        "        \n",
        "        reconstruction_error = np.mean(compute_reconstruction_error(X_gen, autoencoder))\n",
        "        print(f\"Autoencoder reconstruction error (infinity to 0): {reconstruction_error:.3f}\")\n",
        "        print(f\"Counterfactual prediction gain (0 to 1): {delta_clf_pred.mean():.3f}\")\n",
        "        print(f\"Sparsity (L1, infinity to 0): {np.mean(np.abs(X_gen-X_test)):.3f}\")\n",
        "\n",
        "    if weighted_version:\n",
        "        countergan = define_weighted_countergan(generator, discriminator)\n",
        "    else:\n",
        "        countergan = define_countergan(generator, discriminator, classifier)\n",
        "\n",
        "    for iteration in range(n_training_iterations):\n",
        "        if iteration > 0:\n",
        "            x_generated = generator.predict(x_fake_input)\n",
        "            if check_divergence(x_generated):\n",
        "                print(\"Training diverged with the following loss functions:\")\n",
        "                print(discrim_loss_1, discrim_accuracy, gan_loss, \n",
        "                    discrim_loss, discrim_loss_2, clf_loss)\n",
        "                break\n",
        "\n",
        "        # Periodically print and plot training information \n",
        "        if (iteration % 1000 == 0) or (iteration == n_training_iterations - 1):\n",
        "            print_training_information(generator, classifier, X_test, iteration)\n",
        "\n",
        "        # Train the discriminator\n",
        "        discriminator.trainable = True\n",
        "        for _ in range(n_discriminator_steps):\n",
        "            x_fake_input, _ = next(batches)\n",
        "            x_fake = generate_fake_samples(x_fake_input, generator)\n",
        "            x_real = x_fake_input\n",
        "\n",
        "            x_batch = np.concatenate([x_real, x_fake])\n",
        "            y_batch = np.concatenate([np.ones(len(x_real)), np.zeros(len(x_fake))])\n",
        "            \n",
        "            # Shuffle real and fake examples\n",
        "            p = np.random.permutation(len(y_batch))\n",
        "            x_batch, y_batch = x_batch[p], y_batch[p]\n",
        "\n",
        "            if weighted_version:\n",
        "                classifier_scores = classifier.predict(x_batch)[:, DESIRED_CLASS]\n",
        "                \n",
        "                # The following update to the classifier scores is needed to have the \n",
        "                # same order of magnitude between real and generated samples losses\n",
        "                real_samples = np.where(y_batch == 1.)\n",
        "                average_score_real_samples = np.mean(classifier_scores[real_samples])\n",
        "                classifier_scores[real_samples] /= average_score_real_samples\n",
        "                \n",
        "                fake_samples = np.where(y_batch == 0.)\n",
        "                classifier_scores[fake_samples] = 1.\n",
        "\n",
        "                discriminator.train_on_batch(\n",
        "                    x_batch, y_batch, sample_weight=classifier_scores\n",
        "                )\n",
        "            else:\n",
        "                discriminator.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "        # Train the generator \n",
        "        discriminator.trainable = False\n",
        "        for _ in range(n_generator_steps):\n",
        "            x_fake_input, _ = next(batches)\n",
        "            y_fake = np.ones(len(x_fake_input))\n",
        "            if weighted_version:\n",
        "                countergan.train_on_batch(x_fake_input, y_fake)\n",
        "            else:\n",
        "                y_target = to_categorical([DESIRED_CLASS] * len(x_fake_input), \n",
        "                                          num_classes=N_CLASSES)\n",
        "                countergan.train_on_batch(x_fake_input, [y_fake, y_target])\n",
        "    return countergan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7IvGXenSWL-"
      },
      "source": [
        "## CounteRGAN: first formulation for differentiable classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "mNQmxFS5SXIH",
        "outputId": "8ab0b191-abd2-44f3-ecd6-f980df21bd1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-12 12:59:38.174498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.238907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.239006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.249637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.249858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.249963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.529017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.529140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.529158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2025-07-12 12:59:38.529232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.529278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2025-07-12 12:59:38.807478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.807654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.807784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.808247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.808319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2025-07-12 12:59:38.808441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:59:38.808515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2025-07-12 12:59:38.829451: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "2025-07-12 12:59:39.228181: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_1_1/kernel/Assign' id:336 op device:{requested: '', assigned: ''} def:{{{node dense_1_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1_1/kernel, dense_1_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-12 12:59:39.576162: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2/Softmax' id:74 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_2/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-12 12:59:39.665315: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_6/Tanh' id:245 op device:{requested: '', assigned: ''} def:{{{node dense_6/Tanh}} = Tanh[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_6/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Training iteration 0 at 2025-07-12 12:59:39.663078\n",
            "Autoencoder reconstruction error (infinity to 0): 6.424\n",
            "Counterfactual prediction gain (0 to 1): -0.003\n",
            "Sparsity (L1, infinity to 0): 0.240\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-12 12:59:39.900275: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_2/mul' id:404 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-12 12:59:39.948474: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/dense_1_1/bias/v/Assign' id:930 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_1_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_1_1/bias/v, training/Adam/dense_1_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-12 12:59:42.256776: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_3/AddN' id:710 op device:{requested: '', assigned: ''} def:{{{node loss_3/AddN}} = AddN[N=3, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul, loss_3/mul_1, model/activity_regularization/ActivityRegularizer/truediv)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "2025-07-12 12:59:42.317045: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/RMSprop/dense_5_1/bias/rms/Assign' id:1277 op device:{requested: '', assigned: ''} def:{{{node training_2/RMSprop/dense_5_1/bias/rms/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/RMSprop/dense_5_1/bias/rms, training_2/RMSprop/dense_5_1/bias/rms/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================\n",
            "Training iteration 1000 at 2025-07-12 13:02:27.942120\n",
            "Autoencoder reconstruction error (infinity to 0): 6.201\n",
            "Counterfactual prediction gain (0 to 1): 0.088\n",
            "Sparsity (L1, infinity to 0): 0.287\n",
            "========================================================================================\n",
            "Training iteration 1999 at 2025-07-12 13:05:11.858309\n",
            "Autoencoder reconstruction error (infinity to 0): 6.207\n",
            "Counterfactual prediction gain (0 to 1): 0.075\n",
            "Sparsity (L1, infinity to 0): 0.224\n",
            "--------------------------------------------------------------------------------\n",
            "Metrics before immutable features projection:\n",
            "{'latency': '2.271 ± 0.033',\n",
            " 'latency_batch': '454.145',\n",
            " 'prediction_gain': '0.070 ± 0.015',\n",
            " 'reconstruction_error': '6.204 ± 0.237',\n",
            " 'sparsity': '8.300 ± 0.357'}\n",
            "--------------------------------------------------------------------------------\n",
            "Metrics after immutable features projection:\n",
            "{'latency': '2.271 ± 0.033',\n",
            " 'latency_batch': '454.145',\n",
            " 'prediction_gain': '0.003 ± 0.002',\n",
            " 'reconstruction_error': '6.176 ± 0.230',\n",
            " 'sparsity': '0.508 ± 0.037'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "discriminator = create_discriminator()\n",
        "generator = create_generator(residuals=True)\n",
        "batches = infinite_data_stream(X_train, y_train, batch_size=256)\n",
        "samples = X_test \n",
        "\n",
        "method_name = \"countergan\"\n",
        "countergan = train_countergan(2, 4, 2000, classifier, discriminator, generator, batches)\n",
        "\n",
        "t_initial = time.time()\n",
        "counterfactuals = generator.predict(X_test)\n",
        "batch_latency = 1000*(time.time() - t_initial)\n",
        "\n",
        "latencies = np.zeros(len(X_test))\n",
        "for i, x in enumerate(X_test):\n",
        "    t_initial = time.time()\n",
        "    _ = generator.predict(np.expand_dims(x, axis=0))\n",
        "    latencies[i] = 1000*(time.time() - t_initial)\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(\"Metrics before immutable features projection:\")\n",
        "pprint(compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None))\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Set immutable features to original values\n",
        "counterfactuals[:, len(mutable_features):] = samples[:, len(mutable_features):]\n",
        "\n",
        "print(\"Metrics after immutable features projection:\")\n",
        "save_experiment(method_name, X_test, counterfactuals, latencies, batch_latency)\n",
        "\n",
        "generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.h5\", save_format='h5')\n",
        "discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.h5\", save_format='h5')\n",
        "countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.h5\", save_format='h5')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "kdd_counterfactuals_diabetes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.9.22)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
