{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz5mmOdOHznl"
      },
      "source": [
        "# Counterfactuals benchmark on tabular datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYNbfZ4CbwI_",
        "outputId": "a152bcac-e593-4f83-e9b5-ffb9412f9cf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-12 12:57:21.503200: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-07-12 12:57:22.023561: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-07-12 12:57:22.023615: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-07-12 12:57:22.026238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-12 12:57:22.248362: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-07-12 12:57:24.204004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/ahmed/prototype\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.compat.v1 import enable_eager_execution\n",
        "from tensorflow import executing_eagerly\n",
        "import os\n",
        "enable_eager_execution()\n",
        "\n",
        "BASE_PATH = \"./counterfactuals\"\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7sVjpCaH3yD"
      },
      "source": [
        "## Imports and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tW1SdKmbrbj",
        "outputId": "eb57f82e-de9d-4046-de2d-e154bdea3863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alibi version: 0.9.7.dev0\n",
            "Is TensorFlow running in eager execution mode? -----→ True\n",
            "GPU 0: NVIDIA GeForce RTX 4060 Laptop GPU (UUID: GPU-ed7340f2-1910-df12-4a83-29feeba52695)\n"
          ]
        }
      ],
      "source": [
        "# Install the dev version of the Alibi package if not already installed\n",
        "try:\n",
        "    from alibi import __version__ as alibi_version\n",
        "    print(f\"Alibi version: {alibi_version}\")\n",
        "except ImportError:\n",
        "    print(\"Alibi package not found, installing...\")\n",
        "    # Install the dev version of Alibi\n",
        "    !pip install git+https://github.com/SeldonIO/alibi.git > /dev/null\n",
        "\n",
        "\n",
        "import logging\n",
        "\n",
        "alibi_logger = logging.getLogger(\"alibi\")\n",
        "alibi_logger.setLevel(\"CRITICAL\")\n",
        "\n",
        "\n",
        "print(f\"Is TensorFlow running in eager execution mode? -----→ {executing_eagerly()}\")\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6aJpXl3siWZr"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    os.makedirs(BASE_PATH)\n",
        "\n",
        "\n",
        "date = datetime.now().strftime('%Y-%m-%d')\n",
        "EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_{date}\"\n",
        "MODELS_EXPERIMENT_PATH = f\"{BASE_PATH}/diabetes_2020-09-09\"\n",
        "if not os.path.exists(EXPERIMENT_PATH):\n",
        "    os.makedirs(EXPERIMENT_PATH)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ne3HvYI8dv"
      },
      "source": [
        "## Data import and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "mAPAZpVUiks-",
        "outputId": "f7628cfa-2bd6-4c90-e027-d90f0ecf58b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /home/ahmed/prototype\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# import pickle\n",
        "# import time\n",
        "# from matplotlib import offsetbox\n",
        "# from matplotlib.colors import ListedColormap\n",
        "# import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.layers import Dense, Add, Input, ActivityRegularization, Concatenate, Multiply\n",
        "from tensorflow.keras import optimizers, Model, regularizers, Input\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.random import set_seed\n",
        "# from tensorflow.keras.models import load_model\n",
        "import os\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "INITIAL_CLASS = 0\n",
        "DESIRED_CLASS = 1\n",
        "N_CLASSES = 2\n",
        "n_training_iterations = 10\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "# German Credit dataset\n",
        "\n",
        "def preprocess_data_german(df, target_column=\"Outcome\"):\n",
        "    \"\"\"\n",
        "    Preprocess the German Credit dataset by encoding categorical variables and splitting the data into \n",
        "    train, test, and user simulation sets.\n",
        "    \n",
        "    Returns a dictionary with processed train, test, and user datasets.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Assign meaningful column names\n",
        "    df.columns = [\n",
        "        'Status', 'Month', 'Credit_History', 'Purpose', 'Credit_Amount',\n",
        "        'Savings', 'Employment', 'Installment_Rate', 'Personal_Status', 'Other_Debtors',\n",
        "        'Residence_Duration', 'Property', 'Age', 'Other_Installment_Plans', 'Housing',\n",
        "        'Existing_Credits', 'Job', 'Num_Liable_People', 'Telephone', 'Foreign_Worker',\n",
        "        'Outcome'\n",
        "    ]\n",
        "    \n",
        "    # Mapping categorical features to more meaningful values\n",
        "    status_mapping = { 'A11': '< 0 DM', 'A12': '0 <= ... < 200 DM', 'A13': '>= 200 DM / salary assignments for at least 1 year', 'A14': 'no checking account' }\n",
        "    credit_history_mapping = { 'A30': 'no credits taken/ all credits paid back duly', 'A31': 'all credits at this bank paid back duly', 'A32': 'existing credits paid back duly till now', 'A33': 'delay in paying off in the past', 'A34': 'critical account/other credits existing' }\n",
        "    savings_mapping = { 'A61': '< 100 DM', 'A62': '100 <= ... < 500 DM', 'A63': '500 <= ... < 1000 DM', 'A64': '>= 1000 DM', 'A65': 'unknown/no savings account' }\n",
        "    employment_mapping = { 'A71': 'unemployed', 'A72': '< 1 year', 'A73': '1 <= ... < 4 years', 'A74': '4 <= ... < 7 years', 'A75': '>= 7 years' }\n",
        "    personal_status_mapping = { 'A91': 'male: divorced/separated', 'A92': 'female: divorced/separated/married', 'A93': 'male: single', 'A94': 'male: married/widowed', 'A95': 'female: single' }\n",
        "    other_debtors_mapping = { 'A101': 'none', 'A102': 'co-applicant', 'A103': 'guarantor' }\n",
        "    property_mapping = { 'A121': 'real estate', 'A122': 'building society savings agreement/life insurance', 'A123': 'car or other, not in attribute 6', 'A124': 'unknown/no property' }\n",
        "    other_installment_plans_mapping = { 'A141': 'bank', 'A142': 'stores', 'A143': 'none' }\n",
        "    housing_mapping = { 'A151': 'rent', 'A152': 'own', 'A153': 'for free' }\n",
        "    telephone_mapping = { 'A191': 'none', 'A192': 'yes, registered under the customer\\'s name' }\n",
        "    foreign_worker_mapping = { 'A201': 'yes', 'A202': 'no' }\n",
        "\n",
        "    # Apply mappings\n",
        "    df['Status'] = df['Status'].map(status_mapping)\n",
        "    df['Credit_History'] = df['Credit_History'].map(credit_history_mapping)\n",
        "    df['Savings'] = df['Savings'].map(savings_mapping)\n",
        "    df['Employment'] = df['Employment'].map(employment_mapping)\n",
        "    df['Personal_Status'] = df['Personal_Status'].map(personal_status_mapping)\n",
        "    df['Other_Debtors'] = df['Other_Debtors'].map(other_debtors_mapping)\n",
        "    df['Property'] = df['Property'].map(property_mapping)\n",
        "    df['Other_Installment_Plans'] = df['Other_Installment_Plans'].map(other_installment_plans_mapping)\n",
        "    df['Housing'] = df['Housing'].map(housing_mapping)\n",
        "    df['Telephone'] = df['Telephone'].map(telephone_mapping)\n",
        "    df['Foreign_Worker'] = df['Foreign_Worker'].map(foreign_worker_mapping)\n",
        "\n",
        "    # Encode ordinal columns\n",
        "    ordinal_cols = ['Status', 'Credit_History', 'Savings', 'Employment']\n",
        "    le = LabelEncoder()\n",
        "    for col in ordinal_cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "\n",
        "    # One-hot encode nominal columns\n",
        "    nominal_columns = ['Purpose', 'Personal_Status', 'Other_Debtors', 'Property', \n",
        "                       'Other_Installment_Plans', 'Housing', 'Job', 'Telephone', 'Foreign_Worker']\n",
        "    df = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
        "\n",
        "    # Process target variable\n",
        "    Y = df[target_column].replace(1, 0).replace(2, 1)\n",
        "    X = df.drop(columns=[target_column])\n",
        "\n",
        "    # Get final feature set\n",
        "    # list all features\n",
        "    immutable_features = set(X.columns) - set(['Status', 'Credit_History'])\n",
        "    \n",
        "\n",
        "    mutable_features = set(X.columns) - set(immutable_features)\n",
        "    mutable_features = list(mutable_features)\n",
        "\n",
        "    features = list(mutable_features) + list(immutable_features)\n",
        "\n",
        "    return  X, Y, features, immutable_features, mutable_features\n",
        "    \n",
        "# =========================================================\n",
        "\n",
        "\n",
        "# Make sure 'german.csv' is in your project directory\n",
        "df = pd.read_csv('statlog_german_credit_data/german.data', sep=' ', skiprows=1, header=None)\n",
        "x,y, features, immutable_features, mutable_features = preprocess_data_german(df)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=2020)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "X_train = standard_scaler.fit_transform(X_train)\n",
        "X_test = standard_scaler.transform(X_test)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vTcm3LgfKEtl"
      },
      "outputs": [],
      "source": [
        "def compute_reconstruction_error(x, autoencoder):\n",
        "    \"\"\"Compute the reconstruction error for a given autoencoder and data points.\"\"\"\n",
        "    preds = autoencoder.predict(x)\n",
        "    preds_flat = preds.reshape((preds.shape[0], -1))\n",
        "    x_flat = x.reshape((x.shape[0], -1))\n",
        "    return np.linalg.norm(x_flat - preds_flat, axis=1)\n",
        "\n",
        "def format_metric(metric):\n",
        "    \"\"\"Return a formatted version of a metric, with the confidence interval.\"\"\"\n",
        "    return f\"{metric.mean():.3f} ± {1.96*metric.std()/np.sqrt(len(metric)):.3f}\"\n",
        "\n",
        "def compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder,\n",
        "                    batch_latency=None):\n",
        "    \"\"\" Summarize the relevant metrics in a dictionary. \"\"\"\n",
        "    reconstruction_error = compute_reconstruction_error(counterfactuals, autoencoder)\n",
        "    delta = np.abs(samples-counterfactuals)\n",
        "    l1_distances = delta.reshape(delta.shape[0], -1).sum(axis=1)\n",
        "    prediction_gain = (\n",
        "        classifier.predict(counterfactuals)[:, DESIRED_CLASS] - \n",
        "        classifier.predict(samples)[:, DESIRED_CLASS]\n",
        "    )\n",
        "\n",
        "    metrics = dict()\n",
        "    metrics[\"reconstruction_error\"] = format_metric(reconstruction_error)\n",
        "    metrics[\"prediction_gain\"] = format_metric(prediction_gain)\n",
        "    metrics[\"sparsity\"] = format_metric(l1_distances)\n",
        "    metrics[\"latency\"] = format_metric(latencies)\n",
        "    batch_latency = batch_latency if batch_latency else sum(latencies)\n",
        "    metrics[\"latency_batch\"] = f\"{batch_latency:.3f}\"\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def save_experiment(method_name, samples, counterfactuals, latencies, \n",
        "                    batch_latency=None):\n",
        "    \"\"\"Create an experiment folder and save counterfactuals, latencies and metrics.\"\"\"\n",
        "    if not os.path.exists(f\"{EXPERIMENT_PATH}/{method_name}\"):\n",
        "        os.makedirs(f\"{EXPERIMENT_PATH}/{method_name}\")   \n",
        "\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/counterfactuals.npy\", counterfactuals)\n",
        "    np.save(f\"{EXPERIMENT_PATH}/{method_name}/latencies.npy\", latencies)\n",
        "\n",
        "    metrics = compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder)\n",
        "    json.dump(metrics, open(f\"{EXPERIMENT_PATH}/{method_name}/metrics.json\", \"w\"))\n",
        "    pprint(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw26uWLiw2c",
        "outputId": "b89cf614-ecd1-4638-9c6e-0c91a4d81f1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-12 12:57:35.437062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:35.728999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:35.729100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:35.732424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:35.732557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:35.732626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:37.021424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:37.021540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:37.021558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2025-07-12 12:57:37.021633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2025-07-12 12:57:37.021677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "/home/ahmed/prototype/.venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "2025-07-12 12:57:39.980600: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7bda24059640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2025-07-12 12:57:39.980684: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
            "2025-07-12 12:57:39.993627: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2025-07-12 12:57:40.035753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
            "2025-07-12 12:57:40.188975: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: loss=0.2331, accuracy=0.9213\n",
            "Validation: loss=0.7291, accuracy=0.6650\n"
          ]
        }
      ],
      "source": [
        "set_seed(2020)\n",
        "np.random.seed(2020)\n",
        "\n",
        "def create_classifier(input_shape):\n",
        "    \"\"\"Define and compile a neural network binary classifier.\"\"\" \n",
        "    model = Sequential([\n",
        "        Dense(20, activation='relu', input_shape=input_shape),\n",
        "        Dense(20, activation='relu'),\n",
        "        Dense(2, activation='softmax'),\n",
        "    ], name=\"classifier\")\n",
        "    optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    model.compile(optimizer, 'binary_crossentropy', ['accuracy'])\n",
        "    return model\n",
        "\n",
        "classifier = create_classifier((x.shape[1],))\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "sm = SMOTE(random_state=2020)\n",
        "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "y_res = to_categorical(y_res)\n",
        "# Train the classifier\n",
        "training = classifier.fit(X_res, y_res, batch_size=32, epochs=200, verbose=0,\n",
        "                          validation_data=(X_test, y_test),)\n",
        "print(f\"Training: loss={training.history['loss'][-1]:.4f}, \"\n",
        "      f\"accuracy={training.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Validation: loss={training.history['val_loss'][-1]:.4f}, \"\n",
        "      f\"accuracy={training.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "classifier.save(f\"{EXPERIMENT_PATH}/classifier.keras\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.72      0.75       140\n",
            "           1       0.45      0.53      0.49        60\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.62      0.63      0.62       200\n",
            "weighted avg       0.68      0.67      0.67       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gY6FCuwuZ75"
      },
      "source": [
        "## Estimate density with the reconstruction error of a (denoising) autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHvMh2YG3hRK",
        "outputId": "f44437fa-8c44-43c9-8b90-088cfeb07938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.5158\n",
            "Validation loss: 0.6319\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "7/7 [==============================] - 0s 3ms/step\n",
            "{'reconstruction_error': '4.476 ± 0.253',\n",
            " 'reconstruction_error_noise': '5.846 ± 0.048'}\n"
          ]
        }
      ],
      "source": [
        "def add_noise(x, noise_factor=1e-6):\n",
        "    x_noisy = x + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape) \n",
        "    return x_noisy\n",
        "\n",
        "    \n",
        "def create_autoencoder(in_shape=(x.shape[1],)):\n",
        "    input_ = Input(shape=in_shape) \n",
        "\n",
        "    x = Dense(32, activation=\"relu\")(input_)\n",
        "    encoded = Dense(8)(x)\n",
        "    x = Dense(32, activation=\"relu\")(encoded)\n",
        "    decoded = Dense(in_shape[0], activation=\"tanh\")(x)\n",
        "\n",
        "    autoencoder = Model(input_, decoded)\n",
        "    optimizer = optimizers.Nadam()\n",
        "    autoencoder.compile(optimizer, 'mse')\n",
        "    return autoencoder\n",
        "\n",
        "autoencoder = create_autoencoder()\n",
        "training = autoencoder.fit(\n",
        "    add_noise(X_train), X_train, epochs=100, batch_size=32, shuffle=True, \n",
        "    validation_data=(X_test, X_test), verbose=0\n",
        ")\n",
        "print(f\"Training loss: {training.history['loss'][-1]:.4f}\")\n",
        "print(f\"Validation loss: {training.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "n_samples = 1000\n",
        "# Compute the reconstruction error of noise data\n",
        "samples = np.random.randn(n_samples, X_train.shape[1])\n",
        "reconstruction_error_noise = compute_reconstruction_error(samples, autoencoder)\n",
        "\n",
        "# Save and print the autoencoder metrics\n",
        "reconstruction_error = compute_reconstruction_error(X_test, autoencoder)\n",
        "autoencoder_metrics = {\n",
        "    \"reconstruction_error\": format_metric(reconstruction_error),\n",
        "    \"reconstruction_error_noise\": format_metric(reconstruction_error_noise),\n",
        "}\n",
        "json.dump(autoencoder_metrics, open(f\"{EXPERIMENT_PATH}/autoencoder_metrics.json\", \"w\"))\n",
        "pprint(autoencoder_metrics)\n",
        "\n",
        "autoencoder.save(f\"{EXPERIMENT_PATH}/autoencoder.keras\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "kdd_counterfactuals_diabetes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
