{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz5mmOdOHznl"
   },
   "source": [
    "# Counterfactuals benchmark on tabular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21292,
     "status": "ok",
     "timestamp": 1718191682834,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "vYNbfZ4CbwI_",
    "outputId": "0bade714-8815-45d3-ec04-6e356af92d7c"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"./counterfactuals\"\n",
    "#set working directory Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7sVjpCaH3yD"
   },
   "source": [
    "## Import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# from tensorflow import randoms\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Add, Input, ActivityRegularization, Concatenate, Multiply # type: ignore\n",
    "from tensorflow.keras import optimizers, Model, regularizers # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1718191722445,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "6aJpXl3siWZr"
   },
   "outputs": [],
   "source": [
    "def ensure_directory(path):\n",
    "    \"\"\"Ensure the directory exists, if not, create it.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "ensure_directory(BASE_PATH)\n",
    "\n",
    "\n",
    "date = datetime.now().strftime('%Y-%m-%d')\n",
    "EXPERIMENT_PATH = f\"{BASE_PATH}/German_{date}\"\n",
    "ensure_directory(EXPERIMENT_PATH)\n",
    "#https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\\        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1718191743878,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "mAPAZpVUiks-",
    "outputId": "ac0247a5-40c6-4bf2-9c5b-e0833a22165e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up\n",
    "INITIAL_CLASS = 0\n",
    "DESIRED_CLASS = 1\n",
    "N_CLASSES = 2\n",
    "target_column = \"Outcome\"\n",
    "data_source = \"pima\" # \"pima\" or \"german\"\n",
    "\n",
    "np.set_printoptions(precision=2) \n",
    "np.random.seed(2020)\n",
    "\n",
    "# =========================================================\n",
    "# Step 1: Define the Preprocessing Function\n",
    "# =========================================================\n",
    "\n",
    "def preprocess_data_german(df, target_column=\"Outcome\"):\n",
    "    \"\"\"\n",
    "    Preprocess the German Credit dataset by encoding categorical variables and splitting the data into \n",
    "    train, test, and user simulation sets.\n",
    "    \n",
    "    Returns a dictionary with processed train, test, and user datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assign meaningful column names\n",
    "    df.columns = [\n",
    "        'Status', 'Month', 'Credit_History', 'Purpose', 'Credit_Amount',\n",
    "        'Savings', 'Employment', 'Installment_Rate', 'Personal_Status', 'Other_Debtors',\n",
    "        'Residence_Duration', 'Property', 'Age', 'Other_Installment_Plans', 'Housing',\n",
    "        'Existing_Credits', 'Job', 'Num_Liable_People', 'Telephone', 'Foreign_Worker',\n",
    "        'Outcome'\n",
    "    ]\n",
    "    \n",
    "    # Mapping categorical features to more meaningful values\n",
    "    status_mapping = { 'A11': '< 0 DM', 'A12': '0 <= ... < 200 DM', 'A13': '>= 200 DM / salary assignments for at least 1 year', 'A14': 'no checking account' }\n",
    "    credit_history_mapping = { 'A30': 'no credits taken/ all credits paid back duly', 'A31': 'all credits at this bank paid back duly', 'A32': 'existing credits paid back duly till now', 'A33': 'delay in paying off in the past', 'A34': 'critical account/other credits existing' }\n",
    "    savings_mapping = { 'A61': '< 100 DM', 'A62': '100 <= ... < 500 DM', 'A63': '500 <= ... < 1000 DM', 'A64': '>= 1000 DM', 'A65': 'unknown/no savings account' }\n",
    "    employment_mapping = { 'A71': 'unemployed', 'A72': '< 1 year', 'A73': '1 <= ... < 4 years', 'A74': '4 <= ... < 7 years', 'A75': '>= 7 years' }\n",
    "    personal_status_mapping = { 'A91': 'male: divorced/separated', 'A92': 'female: divorced/separated/married', 'A93': 'male: single', 'A94': 'male: married/widowed', 'A95': 'female: single' }\n",
    "    other_debtors_mapping = { 'A101': 'none', 'A102': 'co-applicant', 'A103': 'guarantor' }\n",
    "    property_mapping = { 'A121': 'real estate', 'A122': 'building society savings agreement/life insurance', 'A123': 'car or other, not in attribute 6', 'A124': 'unknown/no property' }\n",
    "    other_installment_plans_mapping = { 'A141': 'bank', 'A142': 'stores', 'A143': 'none' }\n",
    "    housing_mapping = { 'A151': 'rent', 'A152': 'own', 'A153': 'for free' }\n",
    "    telephone_mapping = { 'A191': 'none', 'A192': 'yes, registered under the customer\\'s name' }\n",
    "    foreign_worker_mapping = { 'A201': 'yes', 'A202': 'no' }\n",
    "\n",
    "    # Apply mappings\n",
    "    df['Status'] = df['Status'].map(status_mapping)\n",
    "    df['Credit_History'] = df['Credit_History'].map(credit_history_mapping)\n",
    "    df['Savings'] = df['Savings'].map(savings_mapping)\n",
    "    df['Employment'] = df['Employment'].map(employment_mapping)\n",
    "    df['Personal_Status'] = df['Personal_Status'].map(personal_status_mapping)\n",
    "    df['Other_Debtors'] = df['Other_Debtors'].map(other_debtors_mapping)\n",
    "    df['Property'] = df['Property'].map(property_mapping)\n",
    "    df['Other_Installment_Plans'] = df['Other_Installment_Plans'].map(other_installment_plans_mapping)\n",
    "    df['Housing'] = df['Housing'].map(housing_mapping)\n",
    "    df['Telephone'] = df['Telephone'].map(telephone_mapping)\n",
    "    df['Foreign_Worker'] = df['Foreign_Worker'].map(foreign_worker_mapping)\n",
    "\n",
    "    # Encode ordinal columns\n",
    "    ordinal_cols = ['Status', 'Credit_History', 'Savings', 'Employment']\n",
    "    le = LabelEncoder()\n",
    "    for col in ordinal_cols:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    # One-hot encode nominal columns\n",
    "    nominal_columns = ['Purpose', 'Personal_Status', 'Other_Debtors', 'Property', \n",
    "                       'Other_Installment_Plans', 'Housing', 'Job', 'Telephone', 'Foreign_Worker']\n",
    "    df = pd.get_dummies(df, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "    # Process target variable\n",
    "    Y = df[target_column].replace(1, 0).replace(2, 1)\n",
    "    X = df.drop(columns=[target_column])\n",
    "\n",
    "    # Get final feature set\n",
    "    # list all features\n",
    "    immutable_features = set(X.columns) - set(['Status', 'Credit_History'])\n",
    "    \n",
    "\n",
    "    mutable_features = set(X.columns) - set(immutable_features)\n",
    "    mutable_features = list(mutable_features)\n",
    "\n",
    "    features = list(mutable_features) + list(immutable_features)\n",
    "\n",
    "    return  X, Y, features, immutable_features, mutable_features\n",
    "    \n",
    "# =========================================================\n",
    "\n",
    "\n",
    "def preprocess_data_pima(df, target_column=\"Outcome\"):\n",
    "    \"\"\"\n",
    "    Preprocess the Pima Indians Diabetes dataset:\n",
    "    - Handle missing/zero values for certain features\n",
    "    - Standardize numeric features\n",
    "    - Return train-ready feature matrix, target, and feature info\n",
    "    \"\"\"\n",
    "\n",
    "    # # Replace zeros with NaN for features where zero is not physiologically meaningful\n",
    "    # zero_na_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "    # df[zero_na_columns] = df[zero_na_columns].replace(0, np.nan)\n",
    "    \n",
    "    # # Optionally: impute missing values with median\n",
    "    # df[zero_na_columns] = df[zero_na_columns].fillna(df[zero_na_columns].median())\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    Y = df[target_column]\n",
    "\n",
    "    # Normalize/scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    immutable_features = {\"Pregnancies\", \"DiabetesPedigreeFunction\", \"Age\"}\n",
    "    mutable_features = [col for col in X.columns if col not in immutable_features]\n",
    "\n",
    "    features = mutable_features + list(immutable_features)\n",
    "\n",
    "\n",
    "    return X_scaled, Y, features, immutable_features, mutable_features\n",
    "\n",
    "\n",
    "# Split into train, test, and user sets\n",
    "def split_data(X_features, Y_target, simul_size=0.05, test_size=0.21, random_state=2020):\n",
    "    x_temp, x_user, y_temp, y_user = train_test_split(X_features, Y_target, test_size=simul_size, random_state=random_state)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Convert target variables to categorical\n",
    "    y_train = to_categorical(y_train, num_classes=2)\n",
    "    y_test = to_categorical(y_test, num_classes=2)\n",
    "    y_user = to_categorical(y_user, num_classes=2)\n",
    "    return x_train, x_test, x_user, y_train, y_test, y_user\n",
    "\n",
    "\n",
    "def classify_and_filter_cases(classifier, cases, opposite_CLASS=DESIRED_CLASS):\n",
    "    # Get the predicted probabilities\n",
    "    predicted_probs = classifier.predict_proba(cases)\n",
    "    \n",
    "    # Convert probabilities to class predictions (0 or 1)\n",
    "    predicted_classes = [np.argmax(p, axis=1) for p in predicted_probs][0]\n",
    "\n",
    "    \n",
    "    # Filter cases where the predicted class is the opposite class\n",
    "    opposite_cases = cases[predicted_classes == opposite_CLASS]        \n",
    "    \n",
    "    return opposite_cases\n",
    "\n",
    "# =========================================================\n",
    "# Step 4: Load the Dataset\n",
    "# =========================================================\n",
    "if data_source == \"pima\":\n",
    "    # Load the Pima Indians Diabetes dataset\n",
    "    df = pd.read_csv('pima-indians-diabetes-database/diabetes.csv')\n",
    "    X, Y, features, immutable_features, mutable_features = preprocess_data_pima(df)\n",
    "elif data_source == \"german\":\n",
    "\n",
    "    df = pd.read_csv('statlog_german_credit_data/german.data', delim_whitespace=True, skiprows=1, header=None)\n",
    "    X, Y, features, immutable_features, mutable_features = preprocess_data_german(df)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Step 5: Preprocess the Data\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, x_user, y_train, y_test, y_user = split_data(X, Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1718191853411,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "vTcm3LgfKEtl"
   },
   "outputs": [],
   "source": [
    "def compute_reconstruction_error(x, autoencoder):\n",
    "    \"\"\"Compute the reconstruction error for a given autoencoder and data points.\"\"\"\n",
    "    preds = autoencoder.predict(x)\n",
    "    preds_flat = preds.reshape((preds.shape[0], -1))\n",
    "    x_flat = x.reshape((x.shape[0], -1))\n",
    "    return np.linalg.norm(x_flat - preds_flat, axis=1)\n",
    "\n",
    "def format_metric(metric):\n",
    "    \"\"\"Return a formatted version of a metric, with the confidence interval.\"\"\"\n",
    "    mean_val = metric.mean()\n",
    "    std_val = metric.std()\n",
    "    confidence_interval = 1.96 * std_val / np.sqrt(len(metric))\n",
    "    return f\"{mean_val:.3f} ± {confidence_interval:.3f}\"\n",
    "\n",
    "\n",
    "def compute_metrics (samples, counterfactuals, latencies, classifier, autoencoder, batch_latency=None):\n",
    "    \"\"\"Summarize the relevant metrics of root to leave method in a dictionary.\"\"\"           \n",
    "    \n",
    "    #  Ensure predict_proba returns a NumPy-compatible structure\n",
    "    sample_preds = np.array(classifier.predict_proba(samples))[:, DESIRED_CLASS]\n",
    "    counterfactual_preds = np.array(classifier.predict_proba(counterfactuals))[:, DESIRED_CLASS]\n",
    "\n",
    " \n",
    "    # Compute the metrics\n",
    "    reconstruction_error = compute_reconstruction_error(counterfactuals, autoencoder)\n",
    "    delta = np.abs(samples - counterfactuals)\n",
    "    l1_distances = delta.reshape(delta.shape[0], -1).sum(axis=1)\n",
    "    prediction_gain = counterfactual_preds - sample_preds\n",
    "\n",
    "    # Store results\n",
    "    metrics = {\n",
    "        \"reconstruction_error\": format_metric(reconstruction_error),\n",
    "        \"prediction_gain\": format_metric(prediction_gain),\n",
    "        \"sparsity\": format_metric(l1_distances),\n",
    "        \"latency\": format_metric(latencies),\n",
    "        \"latency_batch\": f\"{(batch_latency or sum(latencies)):.3f}\",\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_experiment(method_name, samples, counterfactuals, latencies,\n",
    "                    classifier, autoencoder, batch_latency=None):\n",
    "    \"\"\"Create an experiment folder and save counterfactuals, latencies, and metrics.\"\"\"\n",
    "    method_path = f\"{EXPERIMENT_PATH}/{method_name}\"\n",
    "    ensure_directory(method_path)\n",
    "\n",
    "    # Save results\n",
    "    np.save(f\"{method_path}/counterfactuals.npy\", counterfactuals)\n",
    "    np.save(f\"{method_path}/latencies.npy\", latencies)\n",
    "\n",
    "    # Compute and save metrics\n",
    "    metrics = compute_metrics(samples, counterfactuals, latencies, classifier, autoencoder)\n",
    "    with open(f\"{method_path}/metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "    pprint(metrics)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ISlxhmjvycC"
   },
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(2020)\n",
    "\n",
    "# Convert data to float32\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "x_test = np.array(x_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Create the classifier\n",
    "classifier = DecisionTreeClassifier(max_depth=5, random_state=2020)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "training = classifier.fit(x_train, y_train)   \n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = classifier.score(x_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Save the decision tree model\n",
    "\n",
    "filename = f\"{EXPERIMENT_PATH}/classifier.sav\"\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gY6FCuwuZ75"
   },
   "source": [
    "## Estimate density with the reconstruction error of a (denoising) autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9819,
     "status": "ok",
     "timestamp": 1718192155575,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "zHvMh2YG3hRK",
    "outputId": "c2f84b49-356a-4da9-8d97-45efaad2204b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2470\n",
      "Validation loss: 0.3038\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "{'reconstruction_error': '1.045 ± 0.164',\n",
      " 'reconstruction_error_noise': '1.018 ± 0.033'}\n"
     ]
    }
   ],
   "source": [
    "# Add noise with dynamic scaling based on the standard deviation of the input\n",
    "def add_noise(x, noise_factor=1e-6):\n",
    "    noise_factor = np.std(x) * noise_factor  # Scale noise by the data's standard deviation\n",
    "    x_noisy = x + (noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x.shape))\n",
    "    return x_noisy\n",
    "\n",
    "\n",
    "def create_autoencoder(in_shape=(x_train.shape[1],)):\n",
    "    \"\"\"Define and compile the autoencoder model with L2 regularization.\"\"\"\n",
    "    input_ = Input(shape=in_shape)\n",
    "\n",
    "    x = Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(input_)  # L2 regularization\n",
    "    encoded = Dense(8)(x)\n",
    "    x = Dense(32, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(encoded)  # L2 regularization\n",
    "    decoded = Dense(in_shape[0], activation=\"tanh\")(x)\n",
    "\n",
    "    autoencoder = Model(input_, decoded)\n",
    "    optimizer = optimizers.Nadam()\n",
    "    autoencoder.compile(optimizer, loss=\"mse\")\n",
    "    return autoencoder\n",
    "\n",
    "# Create and train the autoencoder\n",
    "autoencoder = create_autoencoder(in_shape=(x_train.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(f\"{EXPERIMENT_PATH}/best_autoencoder.keras\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model with early stopping and model checkpoint callbacks\n",
    "training = autoencoder.fit( \n",
    "    x_train, add_noise(x_train), epochs=100, batch_size=32, shuffle=True,\n",
    "    validation_data=(add_noise(x_test), x_test), verbose=0,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Print the final training and validation loss\n",
    "print(f\"Training loss: {training.history['loss'][-1]:.4f}\")\n",
    "print(f\"Validation loss: {training.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Compute the reconstruction error for noise data\n",
    "n_samples = 1000\n",
    "r_samples = np.random.randn(n_samples, x_train.shape[1])\n",
    "reconstruction_error_noise = compute_reconstruction_error(r_samples, autoencoder)\n",
    "\n",
    "# Compute the reconstruction error for the test data\n",
    "reconstruction_error = compute_reconstruction_error(x_test, autoencoder)\n",
    "\n",
    "# Save and print the autoencoder metrics\n",
    "autoencoder_metrics = {\n",
    "    \"reconstruction_error\": format_metric(reconstruction_error),\n",
    "    \"reconstruction_error_noise\": format_metric(reconstruction_error_noise),\n",
    "}\n",
    "json.dump(autoencoder_metrics, open(f\"{EXPERIMENT_PATH}/autoencoder_metrics.json\", \"w\"))\n",
    "pprint(autoencoder_metrics)\n",
    "\n",
    "# Save the model\n",
    "autoencoder.save(f\"{EXPERIMENT_PATH}/autoencoder.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwoisov75MsD"
   },
   "source": [
    "## GAN-based counterfactual search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1718192168894,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "Gi9faGZ42qRR"
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(x, generator):\n",
    "    \"\"\"Use the input generator to generate samples.\"\"\"\n",
    "    return generator.predict(x)\n",
    "\n",
    "def data_stream(x, y=None, batch_size=500):\n",
    "    \"\"\"Generate batches until exhaustion of the input data.\"\"\"\n",
    "    n_train = len(x)  \n",
    "\n",
    "    if y is not None:\n",
    "        assert n_train == len(y)\n",
    "    n_complete_batches, leftover = divmod(n_train, batch_size)\n",
    "    n_batches = n_complete_batches + bool(leftover)\n",
    "\n",
    "    perm = np.random.permutation(n_train)\n",
    "    for i in range(n_batches):\n",
    "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "        if y is not None:\n",
    "            output = (x[batch_idx], y[batch_idx])\n",
    "        else:\n",
    "            output = x[batch_idx]\n",
    "        yield output\n",
    "\n",
    "\n",
    "def infinite_data_stream(x, y=None, batch_size=500):\n",
    "    \"\"\"Infinite batch generator.\"\"\"\n",
    "    batches = data_stream(x, y, batch_size=batch_size)\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(batches)\n",
    "        except StopIteration:\n",
    "            batches = data_stream(x, y, batch_size=batch_size)\n",
    "            yield next(batches)\n",
    "\n",
    "def create_generator(in_shape, mutable_features, residuals=True):\n",
    "    generator_input = Input(shape=in_shape, name='generator_input')\n",
    "    mutable_mask_input = Input(shape=(len(mutable_features),), name='mutable_mask_input')\n",
    "    \n",
    "    # Apply the mutable mask to the generator input\n",
    "    masked_input = Multiply()([generator_input[:, :len(mutable_features)], mutable_mask_input])\n",
    "    \n",
    "    generator = Dense(64, activation='relu')(masked_input)\n",
    "    generator = Dense(32, activation='relu')(generator)\n",
    "    generator = Dense(64, activation='relu')(generator)\n",
    "    generator = Dense(len(mutable_features), activation='tanh')(generator)\n",
    "    generator_output = ActivityRegularization(l1=0., l2=1e-6)(generator)\n",
    "\n",
    "    if residuals:\n",
    "        # Only add the mutable features part\n",
    "        generator_output = Add(name=\"output\")([generator_input[:, :len(mutable_features)], generator_output])\n",
    "\n",
    "    # Concatenate the immutable features part back to the output\n",
    "    immutable_features_part = generator_input[:, len(mutable_features):]\n",
    "    generator_output = Concatenate()([generator_output, immutable_features_part])\n",
    "\n",
    "    return Model(inputs=[generator_input, mutable_mask_input], outputs=generator_output)\n",
    "\n",
    "\n",
    "def create_discriminator(in_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=in_shape))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model  # Ensure this is present!\n",
    "\n",
    "\n",
    "\n",
    "def define_countergan(generator, discriminator,mutable_features):\n",
    "    input_shape = (generator.input_shape[0][1],)  # Ensure input_shape is a tuple\n",
    "    countergan_input = Input(shape=input_shape, name='countergan_input')\n",
    "    mutable_mask_input = Input(shape=(len(mutable_features),), name='mutable_mask_input')\n",
    "\n",
    "    x_generated = generator([countergan_input, mutable_mask_input])\n",
    "    discriminator_output = discriminator(x_generated)\n",
    "\n",
    "    countergan = Model(inputs=[countergan_input, mutable_mask_input], outputs=discriminator_output)\n",
    "\n",
    "    optimizer = optimizers.RMSprop(learning_rate=2e-4)\n",
    "    countergan.compile(optimizer, \"binary_crossentropy\")\n",
    "\n",
    "    return countergan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1718192173366,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "ufnw6FKdPpa2"
   },
   "outputs": [],
   "source": [
    "def train_countergan(n_discriminator_steps, n_generator_steps, n_training_iterations, classifier, discriminator, generator, batches):\n",
    "\n",
    "    def check_divergence(x_generated):\n",
    "        return np.any(np.isnan(x_generated))\n",
    "\n",
    "    def print_training_information(generator, classifier, x_test, iteration):\n",
    "        X_gen = generator.predict([x_test, np.ones((x_test.shape[0], len(features)))])\n",
    "        clf_pred_test = np.array(classifier.predict_proba(x_test))[:, DESIRED_CLASS]\n",
    "        clf_pred = np.array(classifier.predict_proba(X_gen))[:, DESIRED_CLASS]\n",
    "        delta_clf_pred = clf_pred - clf_pred_test\n",
    "        print('='*88)\n",
    "        print(f\"Training iteration {iteration} at {datetime.now()}\")\n",
    "        reconstruction_error = np.mean(np.abs(autoencoder.predict(X_gen) - X_gen))\n",
    "        print(f\"Autoencoder reconstruction error (infinity to 0): {reconstruction_error:.3f}\")\n",
    "        print(f\"Counterfactual prediction gain (0 to 1): {delta_clf_pred.mean():.3f}\")\n",
    "        print(f\"Sparsity (L1, infinity to 0): {np.mean(np.abs(X_gen - x_test)):.3f}\")\n",
    "\n",
    "    countergan = define_countergan(generator, discriminator, mutable_features=features)\n",
    "\n",
    "    for iteration in range(n_training_iterations):\n",
    "        print(f\"Training iteration {iteration} at {datetime.now()}\")\n",
    "        if iteration > 0:\n",
    "            x_generated = generator.predict([x_fake_input, np.ones((x_fake_input.shape[0], len(features)))])\n",
    "            if check_divergence(x_generated):\n",
    "                print(\"Training diverged, stopping early.\")\n",
    "                break\n",
    "\n",
    "        if (iteration % 1000 == 0) or (iteration == n_training_iterations - 1):\n",
    "            print_training_information(generator, classifier, x_test, iteration)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        discriminator.trainable = True\n",
    "        for _ in range(n_discriminator_steps):\n",
    "            x_fake_input, _ = next(batches)\n",
    "            x_fake = generator.predict([x_fake_input, np.ones((x_fake_input.shape[0], len(features)))])\n",
    "            x_real = x_fake_input\n",
    "            x_batch = np.concatenate([x_real, x_fake])\n",
    "            y_batch = np.concatenate([np.ones(len(x_real)), np.zeros(len(x_fake))])\n",
    "            p = np.random.permutation(len(y_batch))\n",
    "            x_batch, y_batch = x_batch[p], y_batch[p]\n",
    "\n",
    "            y_batch = y_batch.reshape(-1, 1)  # Ensure it has shape (512, 1)\n",
    "\n",
    "            discriminator.compile(\n",
    "                optimizer= optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            discriminator.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "        discriminator.trainable = False\n",
    "        for _ in range(n_generator_steps):\n",
    "            x_fake_input, _ = next(batches)\n",
    "            y_fake = np.ones(len(x_fake_input))\n",
    "            countergan.train_on_batch([x_fake_input, np.ones((x_fake_input.shape[0], len(features)))], y_fake)\n",
    "\n",
    "    return countergan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpaZptCP2k_U"
   },
   "source": [
    "## Counterfactual search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115034,
     "status": "ok",
     "timestamp": 1718192292607,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "L4PL01njRnV9",
    "outputId": "226bc316-d7ab-48a8-f9c8-dcdd212be534"
   },
   "outputs": [],
   "source": [
    "def train_gan(method_name, residuals):\n",
    "    samples = classify_and_filter_cases(classifier, x_test, opposite_CLASS=DESIRED_CLASS)\n",
    "    samples = np.array(samples, dtype=np.float32)  # Ensure x_test is a NumPy array with a consistent data type\n",
    "\n",
    "    discriminator = create_discriminator(in_shape=(x_train.shape[1],))\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "       \n",
    "    generator = create_generator(in_shape=(x_train.shape[1],), mutable_features=features, residuals=residuals)\n",
    "    \n",
    "    batches = infinite_data_stream(x_train, y_train, batch_size=256)\n",
    "\n",
    "    countergan = train_countergan(2, 4, 10, classifier, discriminator, generator, batches)\n",
    "   \n",
    "    \n",
    "    # Measure batch latency\n",
    "    t_initial = time.time()\n",
    "    mutable_mask = np.ones((samples.shape[0], len(features)))  # Create a mutable mask\n",
    "    print(mutable_mask)\n",
    "    \n",
    "    counterfactuals = generator.predict([samples, mutable_mask])\n",
    "    batch_latency = 1000 * (time.time() - t_initial)\n",
    "\n",
    "    # Measure latency per sample\n",
    "    latencies = np.zeros(len(samples))\n",
    "    # for i, x in enumerate(samples): ## casing error <shapes [1, 2] and [1, 37]>\n",
    "    #     t_initial = time.time()\n",
    "    #     _ = generator.predict([np.expand_dims(x, axis=0), np.ones(1, len(mutable_features))])\n",
    "    #     latencies[i] = 1000 * (time.time() - t_initial)\n",
    "\n",
    "    save_experiment(method_name, samples, counterfactuals, latencies, classifier, autoencoder, batch_latency)\n",
    "\n",
    "    generator.save(f\"{EXPERIMENT_PATH}/{method_name}/generator.keras\")\n",
    "    discriminator.save(f\"{EXPERIMENT_PATH}/{method_name}/discriminator.keras\")\n",
    "    countergan.save(f\"{EXPERIMENT_PATH}/{method_name}/countergan.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138470,
     "status": "ok",
     "timestamp": 1715164937713,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "mNQmxFS5SXIH",
    "outputId": "335ef678-ec28-4643-9967-8a72691176a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0 at 2025-05-06 17:13:52.445336\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "Training iteration 0 at 2025-05-06 17:13:52.571345\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Autoencoder reconstruction error (infinity to 0): 0.028\n",
      "Counterfactual prediction gain (0 to 1): 0.000\n",
      "Sparsity (L1, infinity to 0): 0.783\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 1 at 2025-05-06 17:13:54.012791\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer._make_function.<locals>.one_step_on_data at 0x000001F11361B7E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer._make_function.<locals>.one_step_on_data at 0x000001F113719F80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training iteration 2 at 2025-05-06 17:13:54.755457\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 3 at 2025-05-06 17:13:55.487526\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 4 at 2025-05-06 17:13:56.237417\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 5 at 2025-05-06 17:13:56.997175\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 6 at 2025-05-06 17:13:57.761181\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Training iteration 7 at 2025-05-06 17:13:58.498591\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 8 at 2025-05-06 17:13:59.469708\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 9 at 2025-05-06 17:14:00.229464\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "========================================================================================\n",
      "Training iteration 9 at 2025-05-06 17:14:00.341840\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Autoencoder reconstruction error (infinity to 0): 0.029\n",
      "Counterfactual prediction gain (0 to 1): 0.000\n",
      "Sparsity (L1, infinity to 0): 0.793\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "{'latency': '0.000 ± 0.000',\n",
      " 'latency_batch': '0.000',\n",
      " 'prediction_gain': '0.000 ± 0.876',\n",
      " 'reconstruction_error': '0.102 ± 0.004',\n",
      " 'sparsity': '6.158 ± 0.469'}\n",
      "Training iteration 0 at 2025-05-06 17:14:01.351341\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "========================================================================================\n",
      "Training iteration 0 at 2025-05-06 17:14:01.484865\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Autoencoder reconstruction error (infinity to 0): 0.226\n",
      "Counterfactual prediction gain (0 to 1): 0.000\n",
      "Sparsity (L1, infinity to 0): 0.157\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Training iteration 1 at 2025-05-06 17:14:02.820693\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 2 at 2025-05-06 17:14:03.604311\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 3 at 2025-05-06 17:14:04.371814\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 4 at 2025-05-06 17:14:05.126303\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 5 at 2025-05-06 17:14:05.879853\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 6 at 2025-05-06 17:14:06.632017\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 7 at 2025-05-06 17:14:07.657329\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 8 at 2025-05-06 17:14:08.425037\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training iteration 9 at 2025-05-06 17:14:09.195273\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "========================================================================================\n",
      "Training iteration 9 at 2025-05-06 17:14:09.303781\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Autoencoder reconstruction error (infinity to 0): 0.235\n",
      "Counterfactual prediction gain (0 to 1): 0.000\n",
      "Sparsity (L1, infinity to 0): 0.170\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "[[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "{'latency': '0.000 ± 0.000',\n",
      " 'latency_batch': '0.000',\n",
      " 'prediction_gain': '0.000 ± 0.000',\n",
      " 'reconstruction_error': '0.948 ± 0.207',\n",
      " 'sparsity': '1.300 ± 0.088'}\n"
     ]
    }
   ],
   "source": [
    "train_gan(method_name=\"regular_gan\", residuals=False)\n",
    "train_gan(method_name=\"countergan\", residuals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdb7em4kTIPE"
   },
   "source": [
    "## Root to leaf counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TExKaAhhj3m-"
   },
   "outputs": [],
   "source": [
    "def tree_branches(model):\n",
    "  n_nodes = model.tree_.node_count\n",
    "  children_left = model.tree_.children_left\n",
    "  children_right = model.tree_.children_right\n",
    "  value = model.tree_.value\n",
    "\n",
    "  all_branches = list(retrieve_branches(n_nodes, children_left, children_right))\n",
    "\n",
    "  # convert all branches to an array with 297 columns (nodes) and 149 rows (branches) with values 0 or 1\n",
    "  branches = np.zeros((len(all_branches), n_nodes))\n",
    "  for index, branch in enumerate(all_branches):\n",
    "      branches[index, branch] = 1\n",
    "\n",
    "  # create an array with leaf nodes and defin the class of each leaf node\n",
    "  leaf_nodes = np.zeros((n_nodes, 2))\n",
    "  for index, branch in enumerate(all_branches):\n",
    "      leaf_nodes[branch[-1], 0] = branch[-1]\n",
    "      if value[branch[-1]][0][0] > value[branch[-1]][0][1]:\n",
    "          leaf_nodes[branch[-1], 1] = 11\n",
    "      else:\n",
    "          leaf_nodes[branch[-1], 1] = 12\n",
    "\n",
    "  return [branches,leaf_nodes]\n",
    "\n",
    "#Retrieve all decision tree branches\n",
    "def retrieve_branches(number_nodes, children_left_list, children_right_list):\n",
    "\n",
    "    # Calculate if a node is a leaf\n",
    "    is_leaves_list = [(False if cl != cr else True) for cl, cr in zip(children_left_list, children_right_list)]\n",
    "\n",
    "    # Store the branches paths\n",
    "    paths = []\n",
    "\n",
    "    for i in range(number_nodes):\n",
    "        if is_leaves_list[i]:\n",
    "            # Search leaf node in previous paths\n",
    "            end_node = [path[-1] for path in paths]\n",
    "\n",
    "            # If it is a leave node yield the path\n",
    "            if i in end_node:\n",
    "                output = paths.pop(np.argwhere(i == np.array(end_node))[0][0])\n",
    "                yield output\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Origin and end nodes\n",
    "            origin, end_l, end_r = i, children_left_list[i], children_right_list[i]\n",
    "\n",
    "            # Iterate over previous paths to add nodes\n",
    "            for index, path in enumerate(paths):\n",
    "                if origin == path[-1]:\n",
    "                    paths[index] = path + [end_l]\n",
    "                    paths.append(path + [end_r])\n",
    "\n",
    "            # Initialize path in first iteration\n",
    "            if i == 0:\n",
    "                paths.append([i, children_left_list[i]])\n",
    "                paths.append([i, children_right_list[i]])\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch(classifier, test_case):\n",
    "    \"\"\"\n",
    "    Retrieves the branch (decision path) for a given test case in the classifier.\n",
    "    \"\"\"\n",
    "    node_indicator = classifier.decision_path(test_case.reshape(1, -1))\n",
    "    branch = node_indicator.indices  # Get node indices in the path\n",
    "    return branch\n",
    "\n",
    "def retrieve_branches_recursive(node, path, children_left, children_right):\n",
    "    \"\"\"\n",
    "    Recursively retrieve all branches in the decision tree.\n",
    "\n",
    "    Parameters:\n",
    "        node (int): Current node index.\n",
    "        path (list): Path of node indices from the root to the current node.\n",
    "        children_left (array): Left child indices for all nodes.\n",
    "        children_right (array): Right child indices for all nodes.\n",
    "\n",
    "    Yields:\n",
    "        list: A branch represented as a list of node indices.\n",
    "    \"\"\"\n",
    "    # Check if the current node is a leaf node (i.e., both left and right children are equal)\n",
    "    if children_left[node] == children_right[node]:  # Leaf node\n",
    "        yield path + [node]  # Yield the path including this leaf node\n",
    "    else:\n",
    "        # Recurse into the left child if it exists\n",
    "        if children_left[node] != -1:\n",
    "            yield from retrieve_branches_recursive(children_left[node], path + [node], children_left, children_right)\n",
    "        # Recurse into the right child if it exists\n",
    "        if children_right[node] != -1:\n",
    "            yield from retrieve_branches_recursive(children_right[node], path + [node], children_left, children_right)\n",
    "\n",
    "\n",
    "def find_similar_branch(branches, test_branch, desired_class_branches):\n",
    "    \"\"\"\n",
    "    Finds the most similar branch to the given test branch among desired class branches.\n",
    "    \"\"\"\n",
    "    # Example similarity: number of overlapping nodes\n",
    "    def similarity_score(branch1, branch2):\n",
    "        return len(set(branch1) & set(branch2))  # Intersection of nodes\n",
    "\n",
    "    # Compare test branch with each desired class branch\n",
    "    most_similar_branch = max(desired_class_branches, key=lambda b: similarity_score(test_branch, b))\n",
    "    return most_similar_branch\n",
    "\n",
    "def generate_synthetic_case(new_branch, original_case, classifier):\n",
    "    \"\"\"\n",
    "    Generates a synthetic test case based on the new branch and the original test case.\n",
    "    \"\"\"\n",
    "    synthetic_case = original_case.copy()\n",
    "    feature = classifier.tree_.feature\n",
    "    threshold = classifier.tree_.threshold\n",
    "\n",
    "    # Update synthetic case based on new branch conditions\n",
    "    for node in new_branch:\n",
    "        if feature[node] != -2:  # Ignore leaf nodes\n",
    "            if original_case[feature[node]] <= threshold[node]:\n",
    "                synthetic_case[feature[node]] = threshold[node] - 0.1\n",
    "            else:\n",
    "                synthetic_case[feature[node]] = threshold[node] + 0.1\n",
    "\n",
    "    return synthetic_case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_to_laef(classifier, x_test, INITIAL_CLASS, DESIRED_CLASS):\n",
    "    # Step 1: Shortlist test cases that are classified as the initial class\n",
    "    test_cases = classify_and_filter_cases(classifier, x_test, INITIAL_CLASS)\n",
    "\n",
    "    # Step 2: Retrieve branches for all desired class cases\n",
    "    all_branches = list(retrieve_branches_recursive(0, [], classifier.tree_.children_left, classifier.tree_.children_right))\n",
    "    desired_class_branches = [branch for branch in all_branches if np.argmax(classifier.tree_.value[branch[-1]][0]) == DESIRED_CLASS]\n",
    "\n",
    "    # Step 3: Process each opposite class case\n",
    "    synthetic_cases = []\n",
    "    \n",
    "    latencies = np.zeros(len(test_cases))    \n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        t_initial = time.time()\n",
    "        test_branch = get_branch(classifier, test_case)\n",
    "        similar_branch = find_similar_branch(all_branches, test_branch, desired_class_branches)\n",
    "        synthetic_case = generate_synthetic_case(similar_branch, test_case, classifier)\n",
    "        synthetic_cases.append(synthetic_case)\n",
    "        latencies[i] = 1000*(time.time() - t_initial)\n",
    "\n",
    "    \n",
    "\n",
    "    return test_cases,np.array(synthetic_cases),latencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "{'latency': '0.105 ± 0.089',\n",
      " 'latency_batch': '5.971',\n",
      " 'prediction_gain': '0.000 ± 0.000',\n",
      " 'reconstruction_error': '2.614 ± 0.335',\n",
      " 'sparsity': '4.517 ± 0.584'}\n"
     ]
    }
   ],
   "source": [
    "method_name = \"root_to_leaf\"\n",
    "samples, counterfactuals, latencies = root_to_laef(classifier, x_test, INITIAL_CLASS, DESIRED_CLASS)\n",
    "\n",
    "save_experiment(method_name, samples, counterfactuals, latencies, classifier, autoencoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T-D0Jj0LVJM"
   },
   "source": [
    "## Generate the benchmark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1715176992055,
     "user": {
      "displayName": "ahmed fares",
      "userId": "04713384456166558102"
     },
     "user_tz": -60
    },
    "id": "5w-2mCNALc8O",
    "outputId": "d8cf6681-72fb-4c6c-dcb0-af2bdb464bec"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "regular_gan",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "countergan",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "root_to_leaf",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a035e697-6c3b-4ed1-b507-c8623c4adaa1",
       "rows": [
        [
         "↓ reconstruction_error (Realism)",
         "0.102 ± 0.004",
         "0.948 ± 0.207",
         "2.614 ± 0.335"
        ],
        [
         "↑ Prediction gain",
         "0.000 ± 0.876",
         "0.000 ± 0.000",
         "0.000 ± 0.000"
        ],
        [
         "↓ Sparsity (Actionability)",
         "6.158 ± 0.469",
         "1.300 ± 0.088",
         "4.517 ± 0.584"
        ],
        [
         "↓ Latency (ms)",
         "0.000 ± 0.000",
         "0.000 ± 0.000",
         "0.105 ± 0.089"
        ],
        [
         "↓ Batch latency (ms)",
         "0.000",
         "0.000",
         "5.971"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regular_gan</th>\n",
       "      <th>countergan</th>\n",
       "      <th>root_to_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>↓ reconstruction_error (Realism)</th>\n",
       "      <td>0.102 ± 0.004</td>\n",
       "      <td>0.948 ± 0.207</td>\n",
       "      <td>2.614 ± 0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>↑ Prediction gain</th>\n",
       "      <td>0.000 ± 0.876</td>\n",
       "      <td>0.000 ± 0.000</td>\n",
       "      <td>0.000 ± 0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>↓ Sparsity (Actionability)</th>\n",
       "      <td>6.158 ± 0.469</td>\n",
       "      <td>1.300 ± 0.088</td>\n",
       "      <td>4.517 ± 0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>↓ Latency (ms)</th>\n",
       "      <td>0.000 ± 0.000</td>\n",
       "      <td>0.000 ± 0.000</td>\n",
       "      <td>0.105 ± 0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>↓ Batch latency (ms)</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    regular_gan     countergan   root_to_leaf\n",
       "↓ reconstruction_error (Realism)  0.102 ± 0.004  0.948 ± 0.207  2.614 ± 0.335\n",
       "↑ Prediction gain                 0.000 ± 0.876  0.000 ± 0.000  0.000 ± 0.000\n",
       "↓ Sparsity (Actionability)        6.158 ± 0.469  1.300 ± 0.088  4.517 ± 0.584\n",
       "↓ Latency (ms)                    0.000 ± 0.000  0.000 ± 0.000  0.105 ± 0.089\n",
       "↓ Batch latency (ms)                      0.000          0.000          5.971"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METHODS = [\"regular_gan\",\"countergan\", \"root_to_leaf\"]\n",
    "# METHODS = [\"root_to_leaf\"]\n",
    "METRIC_NAMES = [\n",
    "    \"reconstruction_error\", \"prediction_gain\", \"sparsity\", \"latency\", \"latency_batch\"\n",
    "]\n",
    "\n",
    "metrics = dict()\n",
    "results = dict()\n",
    "for method in METHODS:\n",
    "    method_metrics = json.load(open(f\"{EXPERIMENT_PATH}/{method}/metrics.json\", \"r\"))\n",
    "    method_metrics = {k: v for k, v in method_metrics.items() if k in METRIC_NAMES}\n",
    "    metrics[method] = method_metrics\n",
    "\n",
    "    results[method] = np.load(f\"{EXPERIMENT_PATH}/{method}/counterfactuals.npy\")\n",
    "    \n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "metrics.index = [\n",
    "    \"↓ reconstruction_error (Realism)\",\n",
    "    \"↑ Prediction gain\",\n",
    "    \"↓ Sparsity (Actionability)\",\n",
    "    \"↓ Latency (ms)\",\n",
    "    \"↓ Batch latency (ms)\",\n",
    "]\n",
    "\n",
    "metrics\n",
    "\n",
    "# Compute the difference between the original test data and the results for each case instanse in the test data \n",
    "def compute_difference(x_test, results):\n",
    "    differences = []\n",
    "    for method, result in results.items():\n",
    "        diff = np.abs(x_test - result)\n",
    "        differences.append(diff)\n",
    "    return differences\n",
    "\n",
    "# differences = compute_difference(x_test, results)\n",
    "# dif1 = pd.DataFrame(differences[0])\n",
    "# dif2 = pd.DataFrame(differences[1])\n",
    "\n",
    "# classifiy dif2 to the desired class\n",
    "dif2_classified = classifier.predict(results[\"countergan\"])\n",
    "dif2_classified = to_categorical(dif2_classified, num_classes=2)\n",
    "dif2_classified = pd.DataFrame(dif2_classified[:,1])\n",
    "\n",
    "dif1_classified = classifier.predict(results[\"regular_gan\"])\n",
    "dif1_classified = to_categorical(dif1_classified, num_classes=2)\n",
    "dif1_classified = pd.DataFrame(dif1_classified[:,1])\n",
    "\n",
    "\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zWMMH-btKLOB"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1feLrrt01BgnJz0qQFIF65RmeCv5D-VET",
     "timestamp": 1685989920022
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
